{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"left\">\n",
    "    <h1>SISTEMAS COMPUTACIONAIS AVANÇADOS (SISTCA) ADVANCED COMPUTING SYSTEMS</h1>\n",
    "    <h2>OpenAI</h2>\n",
    "    <h4>Degree in Telecommunications and Informatics Engineering </h4>\n",
    "    <h5>\n",
    "    <img width=\"500\" src=\"imgs/ISEP_logo.png\"><br>\n",
    "    2023 / 24</h5>\n",
    "</div>\n",
    "\n",
    "<table align=\"left\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Version</th>\n",
    "            <th>Date</th>\n",
    "            <th>Authors</th>\n",
    "            <th>Update information</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>V1.0</td>\n",
    "            <td>April 2024</td>\n",
    "            <td style=\"text-align: left;\">\n",
    "                Patrícia Sousa (1210713), Carlos Alves (1211604), José Leal (1211066), Tiago Ribeiro (1210924)\n",
    "                <br><br>\n",
    "                <strong>Supervisor:</strong> Paula Viana (pmv)\n",
    "            </td>\n",
    "            <td>Original version</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Context\n",
    "\n",
    "In this script we will present an API developed by openAI, one of the leading organisations in artificial intelligence (AI) research and development. Launched in 2020, it  represents a significant milestone in accessing advanced AI technology. It offers an accessible and simplified interface for developers to integrate AI capabilities into a variety of applications, from text analysis to content generation.\n",
    "\n",
    "The OpenAI API is based on recent advances in machine learning, in particular, deep neural network architectures, such as GPT (Generative Pre-trained Transformer) language models. These models have revolutionised the way machines understand and generate natural language, enabling tasks such as automatic translation, text summarising, question answering, and content generation.\n",
    "\n",
    "GPT is trained with a vast amount of textual data from the internet, allowing it to capture the nuances and complexities of human language. Through the pre-training process, the model learns to represent knowledge in a general way, without the need for specific training for a particular task.\n",
    "\n",
    "The OpenAI API leverages these pre-trained language models, providing a simple and effective interface for developers to take advantage of their capabilities. This means users can easily integrate advanced AI capabilities into their applications, without needing to understand complex implementation details or model training.\n",
    "\n",
    "Additionally, it is designed with a focus on security and ethics, including measures to prevent malicious or harmful use of AI technology. This reflects OpenAI's commitment to promoting the responsible development of artificial intelligence and ensuring that its benefits are extremely accessible and used for the good of society.\n",
    "\n",
    "Lastly, for a more interactive experience for the user, this script is available in another format, jupyter notebook. A free open-source software launched in 2014 by IPython that provides a notebook, this is, a shareable document that combines computer code, plain language descriptions, data, rich visualizations like 3D models, charts, graphs and figures, and interactive controls in one file, creating a fast and easy way to prototype and explain code, visualize data, and share ideas.\n",
    "\n",
    "## 1.2 Motivation\n",
    "\n",
    "One of the main motivations for choosing this topic lies in the possibility of structured presenting a language (and a platform) accessible to all students, providing them with the opportunity to work and apply it in the future, as it is an increasingly used topic day to day.\n",
    "Whether in the SISTCA curricular unit or in other contexts, the OpenAI API offers a versatile and robust environment to explore the potential of artificial intelligence.\n",
    "\n",
    "When choosing this topic, the need to familiarise yourself not only with the fundamental concepts of artificial intelligence, but also with the practical tools for its implementation, is recognised. This required not only mastering theoretical principles, but also exploring accessible programming languages and platforms such as the OpenAI API.\n",
    "\n",
    "One of the main motivations for choosing this topic lies in the importance of artificial intelligence in society today, some of whose main impacts include \n",
    "the automation and efficiency of tasks, more accurate medical diagnoses, learning adapted to the needs of each student, as well as the existence of virtual assistants.\n",
    "\n",
    "By integrating artificial intelligence into various aspects of society, it is possible to solve complex problems, improve quality of life and drive innovation. \n",
    "However, it is essential that the development and implementation of AI is carried out in an ethical and responsible manner, taking into account privacy and security issues  \n",
    "to ensure that the benefits are widely distributed and the risks mitigated.\n",
    "\n",
    "Whether in the SISTCA course or in other contexts, the OpenAI API offers a versatile and robust environment for exploring the potential of artificial intelligence.\n",
    "\n",
    "In choosing this subject, it is recognised that there is a need to become familiar not only with the fundamental concepts of artificial intelligence, but also with the practical tools for \n",
    "its implementation. To this end, it is necessary not only to master theoretical principles, but also to explore programming languages and accessible platforms, such as the OpenAI API.\n",
    "\n",
    "Whether in the SISTCA course or in other contexts, the OpenAI API offers a versatile and robust environment for exploring the potential of artificial intelligence.\n",
    "\n",
    "## 1.3 Objectives\n",
    "\n",
    " The main objective of this script is to explore the various features offered by the OpenAI API and learn how to integrate them, promoting a comprehensive and practical understanding of the platform's capabilities, providing users with a solid foundation to explore and use the following available tools:\n",
    "\n",
    "- **Chat Completions**: To understand the generic implementation of a chat-based GPT assistant.\n",
    "- **Assistants API + Tools**: Create a program that provides users with media suggestions such as books, films, or TV shows.\n",
    "- **Embeddings**: Learn how to represent words, sentences, paragraphs, or entire documents in a continuous vector space.\n",
    "- **Vision**: Use Vision functionality to extract information from an image.\n",
    "- **Image Generation/DALL-E**: Learn how to generate AI-created images.\n",
    "- **TTS (Text to Speech)**: Turn strings into robotized speech using AI.\n",
    "- **Whisper**: Capture audio and translate it into different languages.\n",
    "- **Moderation**: Explore OpenAI's Moderation functionality to detect inappropriate content.\n",
    "\n",
    "\n",
    "In addition, two practical exercises will be developed to apply the acquired knowledge and a\n",
    "challenge will be proposed to test the reader’s understanding and ability to use the OpenAI API.\n",
    "\n",
    "## 1.4 Document Structure\n",
    "\n",
    "This tutorial is organised into seven chapters with the following structure: Introduction, Theoretical, Setup/Installation, Tutorial/Functionality, Exercises followed by a Challenge and ends with a closing chapter.\n",
    "    \n",
    "In the Introduction, some concepts about the OpenAI API will be presented that will be deepened throughout the script. \n",
    "\n",
    "The theoretical part, will introduce the State-of-the-art and detail the API's Features. \n",
    "\n",
    "Then, the third chapter focuses on Setup/Installation, explaining how to create an open AI account and set up a development environment. \n",
    "\n",
    "In the Tutorial/Functionality part, a detailed tutorial of the available features will be provided. \n",
    "\n",
    "After all these topics, two exercises will be developed and their corresponding resolution will be presented and a final challenge to test the knowledge acquired by the users. \n",
    "\n",
    "After the tutorials two exercises are proposed, which are followed by a final challenge.  \n",
    "\n",
    "Finally, we conclude by discussing future improvements to the script. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2 Theoretical (scientific/technological background)\n",
    "\n",
    "Before diving into specific AI models it's important to have a general understanding of how AI generally works.\n",
    "\n",
    "Most AI solutions in natural language processing are based on broader **Large language models (LLMs)** these are algorithms that have been trained on vast amounts of data with the purpose of understanding and generating human like text. LLMs make use of the **transformer** type of architecture, a deep learning technique that, in short, represents text via numerical representations known as tokens and gives them different weights so as to be able to contextualise words and find similarities [¹](https://www.ibm.com/topics/ai-model).\n",
    "\n",
    "One example of a transformer based model is **the Generative pre-trained transformer (GPT)**, an AI model that has been pre-trained on large sets of data via the use of the transformer architecture for general purpose tasks. Furthermore, these models can be fine tuned to achieve greater performance in more specific tasks, such is the case of image generation models [²](https://blogs.nvidia.com/blog/what-is-a-transformer-model/).\n",
    "\n",
    "Lastly, in order to make use of these models it's essential to understand the concept of prompts. That is the name given to the textual inputs given to the model. These inputs are then broken down into the aforementioned tokens via a process we call tokenization, which facilitates the use of the models language structure.\n",
    "\n",
    "## 2.1 State-of-the-art\n",
    "\n",
    "AI, Artificial Intelligence, refers to a simulation of human intelligence in machines programmed to mimic human cognitive processes and actions. The concept of AI is not new, it has been around since the mid-20th century, but in the last few years there have been significant advances for AI. With these advances, due to its potential, AI has become increasingly important across various industries such as healthcare, finance, manufacturing, education, amongst others. \n",
    "Nowadays, one of the most famous companies developing AI products is OpenAI, which we are going to base our work on. \n",
    "However, just like any other business, OpenAI has its competitors. Some of them currently only dispose of a chat bot, and even that, at the time, are not available in Portugal. Listed below are some companies in the AI business.\n",
    "\n",
    "\n",
    "| AI                                      | **OpenAI[³](https://openai.com/)**                          | **X[⁴](https://grok.x.ai/)**     | **Anthropic[⁵](https://www.anthropic.com/)**                 | **Deepmind[⁶](https://deepmind.google/technologies/)**         | **Cohere[⁷](https://cohere.com/)**       |\n",
    "|----------------------------------------|-------------------------------------|-----------|-------------------------------|----------------------|------------------|\n",
    "| **ChatBot**                            | ChatGPT                             | Grok      | Claude                        | Gemini               | Coral            |\n",
    "| **API**                                | Yes                                 | No        | Yes                           | Yes                  | Yes              |\n",
    "| **Inputs**                             | Text, Audio, Image, Video          |           | Text                          | Text, Image          | Text             |\n",
    "| **Outputs**                            | Text, Audio, Image, Video          |           | Text                          | Text                 | Text             |\n",
    "| **ChatBot and API Availability in Portugal** | Both                                | None      | API Only                      | ChatBot Only         | Both             |\n",
    "\n",
    "\n",
    "## OpenAI 2.2 Features\n",
    "\n",
    "OpenAI's API offers a vast array of cutting-edge AI models based on deep learning and natural language processing techniques. These models have been trained on vast datasets and fine tuned to fulfil a variety of tasks such as text and image generation, audio and text conversions, amongst other things.\n",
    "\n",
    "### 2.2.1 GPT\n",
    "\n",
    "The GPT (Generative Pre-trained Transformer) series is OpenAI’s main set of large language\n",
    "models. These models are trained to understand and generate natural language text based on\n",
    "contextual inputs so as to better communicate with humans. The most widespread version, GPT-\n",
    "3.5 currently serves as the model that powers the free version of ChatGPT. Its understanding\n",
    "of human language allows for coherent conversations which makes it a suitable chat bot. GPT-4\n",
    "improves upon its predecessor with a smarter and more knowledgeable model that provides greater\n",
    "accuracy across various tasks. In particular, GPT-4 introduces Vision as a new feature, that enables\n",
    "it to process image inputs, making it useful in a wider range of applications.\n",
    "\n",
    "\n",
    "#### Function calling\n",
    "\n",
    "The GPT based models are capable of calling previously specified functions in response to user\n",
    "actions or prompts by calling external APIs to retrieve data or to automate procedures like sending\n",
    "an email or extracting and sorting data from a document.\n",
    "\n",
    "#### Assistants\n",
    "\n",
    "The AI Assistants functionality leverages the use of the GPT models alongside function calling\n",
    "and other tools like file retrieval and code interpreter to allow users to create custom assistants\n",
    "that fulfil more specific tasks based on the provided instructions.\n",
    "\n",
    "### 2.2.2  Other models\n",
    "\n",
    "#### DALL-E\n",
    "\n",
    "The DALL-E model is capable of generating images from natural-language text descriptions, as\n",
    "well as modifying existing ones by feeding the model instructions from a text prompt.\n",
    "\n",
    "#### TTS\n",
    "\n",
    "The Text-To-Speech model is capable of converting text into a natural sounding speech. Note\n",
    "that it currently only supports the English language.\n",
    "\n",
    "#### Whisper\n",
    "\n",
    "Whisper does the opposite of the TTS model: it takes an audio input and then transcribes it\n",
    "into text. Unlike text to speech, Whisper is capable of understanding multiple languages, as such\n",
    "it can be used to identify the input language and translate the contents of the speech into English.\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "Text embeddings are vectorial representations of strings of text, such as words or phrases. By\n",
    "comparing two or more vectors we can infer their similarity. This mechanism is highly useful in\n",
    "applications such as search engines or product recommendations due to it’s ability of evaluating\n",
    "similarity between text strings.\n",
    "\n",
    "#### Moderation\n",
    "\n",
    "OpenAI’s Moderation model is designed to verify if a certain piece of text includes any content\n",
    "that could be classified as hateful, violent, sexual, harmful or otherwise inappropriate. Whilst\n",
    "OpenAI’s own use of the model aims to ensure that content complies with their [usage policies](https://openai.com/),\n",
    "this model is suitable for any application that aims to ensure a safe digital environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Setup/Installation\n",
    "\n",
    "## 3.1 Creating an OpenAI Account\n",
    "\n",
    "In this section,we will guide you through the process of setting up an OpenAI account. Whether\n",
    "you’re a developer, researcher, or simply curious about AI, having your own account opens the\n",
    "door to the vast possibilities of artificial intelligence.  \n",
    "\n",
    "Firstly, navigate to the [OpenAI website](https://openai.com) to create or log into your account.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/setup_imgs/passo1.png\" alt=\"OpenAI Website Access\" width=\"1200\"/>\n",
    "</p>\n",
    "\n",
    "Upon logging in with your email, you will be presented with two options: ChatGPT and API. Select the API option to access the documentation.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/setup_imgs/passo7.png\" alt=\"Select the API Option\"/>\n",
    "</p>\n",
    "\n",
    "Congratulations! You have now successfully created an operational OpenAI account.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/setup_imgs/passo8.png\" alt=\"OpenAI Account Creation Confirmation\" width=\"1200\"/>\n",
    "</p>\n",
    "\n",
    "## 3.2 Setting up Your Development Environment\n",
    "\n",
    "Setting up a proper development environment is crucial for working efficiently with AI appli-\n",
    "cations. Ensure that you have the necessary tools and libraries installed on your system. For most\n",
    "AI development tasks, Python is the recommended programming language due to its extensive\n",
    "ecosystem of AI and machine learning libraries. So, that is exactly what we are going to help you\n",
    "with in this sub-section.\n",
    "\n",
    "### 3.2.1 Windows\n",
    "\n",
    "The first step is accessing python’s official website and downloading it. In case you are not sure\n",
    "if you have already installed Python in the past, just type \"cmd\" in your search-bar and then type\n",
    "\"python\". If you are having trouble installing, maybe try checking [Python’s beginners guide](https://wiki.python.org/moin/BeginnersGuide/Download).\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/setup_imgs/step1.png\" alt=\"OpenAI Account Creation Confirmation\" width=\"1200\"/>\n",
    "</p>\n",
    "\n",
    "Once installed, you are going to create a virtual environment, as it is good practise to avoid\n",
    "conflicts with other installed libraries.  \n",
    "\n",
    "Type this command in your command prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "python3 -m venv openai-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after creating the virtual environment, you need to activate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "openai-env\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step you should be able to see \"openai-env\" to the left of the cursor input section.\n",
    "\n",
    "Once you have Python installed and (optionally) set up a virtual environment, the OpenAI Python library can be installed. From the command prompt, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this completes, running 'pip list' will show you the Python libraries you have installed in your current environment, which should confirm that the OpenAI Python library was successfully installed.\n",
    "\n",
    "Now we are going to setup your API key. If you don't have an API key yet then you'll have to follow the instruction in API section.\n",
    "\n",
    "Open your command prompt and then insert the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "setx OPENAI_API_KEY \"your-api-key-here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this key setup permanent, you ought to access Environment Variables, for that you just need to search for it in your windows search bar. Click on \"New\" and then set `OPENAI_API_KEY` as the variable name and your API key as the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Linux\n",
    "\n",
    "Firstly, open your terminal and introduce the following command in order to download python:\n",
    "\n",
    "In case you are working on Debian or Ubuntu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "apt install python3 python3-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are working on Red Hat, CentOS, or Fedora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "dnf install python3 python3-devel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having trouble installing, maybe try checking [Python's beginners guide](https://wiki.python.org/moin/BeginnersGuide/Download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, you are going to create a virtual environment, as it is good practise to avoid conflicts with other installed libraries.\n",
    "\n",
    "Type this command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m venv openai-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can't use none of these commands above because of this error: \"The virtual environment was not created successfully because ensurepip is not available\" then, try using the following command (after this command you have to re-insert one of the commands above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt install python3.10-venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after creating the virtual environment, you need to activate it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "source openai-env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step you should be able to see \"openai-env\" to the left of the cursor input section.\n",
    "\n",
    "Once you have Python installed and (optionally) set up a virtual environment, the OpenAI Python library can be installed. From the terminal, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this completes, running 'pip list' will show you the Python libraries you have installed in your current environment, which should confirm that the OpenAI Python library was successfully installed.\n",
    "\n",
    "Now we are going to setup your API key. If you don't have an API key yet then you'll have to follow the instruction in API section\n",
    "\n",
    "Go to OpenAI website and access the \"API keys\" section, there you are going to retrieve your API key or create one in case you do not already have.\n",
    "\n",
    "Then, open your terminal and type the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export OPENAI_API_KEY='your-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save just press Ctrl+O.\n",
    "        \n",
    "If you want to check whether it is setup correctly, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"imgs/setup_imgs/step2.png\" alt=\"Setup Python\" width=1200/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 MacOS\n",
    "\n",
    "Firstly install Brew, if not already installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you already have Brew, open your terminal and introduce the following command in order to download python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "brew install python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having trouble installing, maybe try checking [Python's beginners guide](https://wiki.python.org/moin/BeginnersGuide/Download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, you are going to create a virtual environment, as it is good practice to avoid conflicts with other installed libraries.\n",
    "\n",
    "Type this command in your terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install virtualenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create venv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "virtualenv openai-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after creating the virtual environment, you need to activate it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "source openai-env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step you should be able to see \"openai-env\" to the left of the cursor input section.\n",
    "\n",
    "Once you have Python installed and (optionally) set up a virtual environment, the OpenAI Python library can be installed. From the terminal, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this completes, running 'pip list' will show you the Python libraries you have installed in your current environment, which should confirm that the OpenAI Python library was successfully installed.\n",
    "\n",
    "Now we are going to setup your API key. If you don't have an API key yet then you'll have to follow the instruction in API section\n",
    "\n",
    "Go to OpenAI website and access the \"API keys\" section, there you are going to retrieve your API key or create one in case you do not already have.\n",
    "\n",
    "Then, open your terminal and type the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export OPENAI_API_KEY='your-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save just press Ctrl+O.\n",
    "    \n",
    "If you want to check it did get setup correctly, type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Installing Jupyter\n",
    "\n",
    "The tutorials in this script have been designed to be used with [Jupyter](https://jupyter.org/), an interactive python environment that will ease your first steps into AI.\n",
    "\n",
    "You can install JupyterLab, the web-based interface, by running the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can launch the interface with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using an IDE such as `VS CODE` or `Jetbrain's PyStorm` simply install the available jupyter notebook extension/plugin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required packages\n",
    "\n",
    "Before starting the tutorials, install all of the necessary python packages using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Tutorial/Functionality\n",
    "\n",
    "In order to work with OpenAI's APIs you must first import it's library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this library you need to instantiate a client. To do this you are going to need a key. If no key is indicated in the constructor, OpenAI will default to the environment variable `OPEN_AI_KEY` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY -> is retrieved from the environment variable\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY -> is indicated by the user at the moment of creation\n",
    "client = OpenAI(api_key=\"YOUR_OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good practice, we advise that you use a `.env` file to store your private information, like your key (see `.env.example`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Client with .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Chat Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Chat Completions is probably one of the simplest OpenAI´s capabilities. It simply takes a list of messages as input and returns an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to structure our response, passing the gpt model and a few messages to give a brief context to our chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful football assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the Euro back in 2016?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Portugal won the World Cup in 2016.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played, what was the score of the final and who scored in that game?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assistants API makes it possible to create AI assistants within applications. An Assistant has instructions and can use models, tools and files to answer the user's questions. The Assistants API currently supports three types of tools: Code Interpreter, File Search and Function Calling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Assistant\n",
    "\n",
    "In this example, an Assistant is created with the purpose of being a personal maths tutor.\n",
    "\n",
    "Initially, an Assistant is created, which represents an entity that can be configured to respond to a user's messages using various parameters such as a , `name`, `instructions` and `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a Thread is created that represents a conversation between a user and one or more Assistants. A Thread can be created when a user (or their AI application) starts a conversation with their Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message is added to the Thread, the content of the messages that users or applications create is added as objects to the Message Thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation: 4a - 13 = 7.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user has added all of the necessary information/messages to the thread you must run the thread to provide them with results.\n",
    "\n",
    "You can run a thread using its id as well as the assistant's id, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    #model = \"gpt-3.5-turbo\" # By default the model defined in the assistant will be used. You can, however, overwrite it here.\n",
    "    instructions = \"Please address the user as SISTCA student.\" # This parameter is optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During its lifecycle, a run can have multiple states:\n",
    "\n",
    "![image.png](https://cdn.openai.com/API/docs/images/diagram-run-statuses-v2.png)\n",
    "[https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps]\n",
    "\n",
    "Therefore, we must check for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == 'completed': \n",
    "    messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "    )\n",
    "    print(messages)\n",
    "else:\n",
    "    print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is then returned by printing the messages returned by the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id = thread.id)\n",
    "print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to yield any results, an assistant's thread must be run. This is to say that whenever you're working with the assistant functionality you must have a runner function. \n",
    "\n",
    "Due to the fact that we will be exploring other types of assistants, and as a way of not repeating this code multiple times, we will export as the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_run (thread_id, assistant_id, client):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread_id,\n",
    "        assistant_id = assistant_id,\n",
    "        instructions = \"Please address the user as SISTCA student.\"\n",
    "    )   \n",
    "\n",
    "    if run.status == 'completed': \n",
    "        messages = client.beta.threads.messages.list(thread_id = thread_id)\n",
    "        print(messages.data[0].content[0].text.value)\n",
    "    else:\n",
    "        print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have used ChatGPT before you may have noticed that the output is not returned all at once, like it is here, but rather gradually, in real time.\n",
    "\n",
    "We can achieve this same effect by using a different type of runner, the streaming run.\n",
    "\n",
    "\n",
    "In this run, the streaming_run function is defined, which is responsible for starting the streaming with the assistant. \n",
    "Within this, the 'EventHandler' class is defined, which is inherited from AssistantEventHandler. \n",
    "This class contains the methods responsible for dealing with the different events that occur during streaming: \n",
    "\n",
    "* `on_text_created`: Called when a new text is created by the assistant. This prints 'assistant' to indicate that the assistant is responding.\n",
    "\n",
    "* `on_text_delta`: Called for each text update. This prints the part of the text that was generated (delta.value).\n",
    "\n",
    "* `on_tool_call_created`: Called when the wizard calls a tool. This prints the type of tool call (tool_call.type).\n",
    "\n",
    "* `on_tool_call_delta`:  Called for tool call updates, specifically for the code_interpreter. This prints the code_interpreter input and any outputs, especially logs.\n",
    "\n",
    "\n",
    "Finally, the API client's create_and_stream method is used to start streaming, passing the EventHandler as the event handler.\n",
    "\n",
    "\n",
    "* `thread_id`: and`assistant_id` are the required identifiers.\n",
    "* `instructions` defines specific instructions for the assistant, in this case asking it to address the user as a SISTCA student.\n",
    "* `event_handler` is the instance of the EventHandler class.\n",
    "\n",
    "The `with` block ensures that streaming continues until it is finished (stream.until_done())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "def streaming_run (thread_id, assistant_id, client):\n",
    "\n",
    "    # First, we create a EventHandler class to define\n",
    "    # how we want to handle the events in the response stream.\n",
    "    \n",
    "    class EventHandler(AssistantEventHandler):    \n",
    "        @override\n",
    "        def on_text_created(self, text) -> None:\n",
    "            print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "            \n",
    "        @override\n",
    "        def on_text_delta(self, delta, snapshot):\n",
    "            print(delta.value, end=\"\", flush=True)\n",
    "            \n",
    "        def on_tool_call_created(self, tool_call):\n",
    "            print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "        \n",
    "        def on_tool_call_delta(self, delta, snapshot):\n",
    "            if delta.type == 'code_interpreter':\n",
    "                if delta.code_interpreter.input:\n",
    "                    print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "\n",
    "    # Then, we use the `create_and_stream` SDK helper \n",
    "    # with the `EventHandler` class to create the Run \n",
    "    # and stream the response.\n",
    "    \n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id,\n",
    "        instructions=\"Please address the user as SISTCA student.\",\n",
    "        event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(streaming_run(thread.id, assistant.id, client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Function Calling\n",
    "\n",
    "Function Calling is one of Assistants API's tools, which in simple terms, invokes predefined functions in order to respond to a certain prompt. As you will verify in just a moment, in this tutorial we asked our weather assistant to give us the maximum and minimum temperatures and rain probability in a certain location.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using functions we allow the assistant to call additional methods as it sees fit. Assistants will analyse the user's message and decide if there is a function that can be used to answer the request and, if so, a request to call that function will be made to the runner.\n",
    "\n",
    "For instance, in this case we want to make it so that when asking for weather data for a Portuguese city or region the data should come from [IPMA](https://www.ipma.pt/pt/index.html).\n",
    "\n",
    "By using [IPMA'S API](https://api.ipma.pt/) we can check the weather data for a specific location using it's ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the functions the assistant may call, alongside their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are a weather bot. Use the provided functions to answer questions.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather_data\",\n",
    "                \"description\": \"Get the current weather forecast for the specified location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City or region for which to get the weather forecast\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we must define actual functions to fetch the weather data.\n",
    "\n",
    "To get weather data for a city or region we must first know its global ID. We can do this be accessing the [list of identifiers](https://api.ipma.pt/open-data/distrits-islands.json) and extracting the `globalIdLocal` for the desired city (`location_name`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visiting the [list of identifiers endpoint](https://api.ipma.pt/open-data/distrits-islands.json), this json is returned:\n",
    "``` json\n",
    "{\n",
    "    \"owner\": \"IPMA\",\n",
    "    \"country\": \"PT\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"idRegiao\": 1,\n",
    "            \"idAreaAviso\": \"AVR\",\n",
    "            \"idConcelho\": 5,\n",
    "            \"globalIdLocal\": 1010500,\n",
    "            \"latitude\": \"40.6413\",\n",
    "            \"idDistrito\": 1,\n",
    "            \"local\": \"Aveiro\",\n",
    "            \"longitude\": \"-8.6535\"\n",
    "        },\n",
    "        {\n",
    "            \"idRegiao\": 1,\n",
    "            \"idAreaAviso\": \"BJA\",\n",
    "            \"idConcelho\": 5,\n",
    "            \"globalIdLocal\": 1020500,\n",
    "            \"latitude\": \"38.0200\",\n",
    "            \"idDistrito\": 2,\n",
    "            \"local\": \"Beja\",\n",
    "            \"longitude\": \"-7.8700\"\n",
    "        },\n",
    "    [...]    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the json structure, define a function to get the globalIdLocal for the matching city: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_location_id(location_name):\n",
    "    url = \"https://api.ipma.pt/open-data/distrits-islands.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for location in data[\"data\"]:\n",
    "                # Check if the location name matches the input by converting them to lower case, so that i.e. \"Porto\" == \"porto\"\n",
    "                if location[\"local\"].lower() == location_name.lower():\n",
    "                    return str(location[\"globalIdLocal\"])\n",
    "            return None  # Location not found\n",
    "        else:\n",
    "            print(\"Failed to fetch data:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function:\n",
    "location_id = get_location_id(\"Porto\")\n",
    "print(location_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function, `get_weather_data()`, uses the `location_id` returned by `get_location_id()` to access the actual weather data of the required location, which we then return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(location_id):\n",
    "    url = f\"https://api.ipma.pt/open-data/forecast/meteorology/cities/daily/{location_id}.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            forecast = data[\"data\"][0]\n",
    "            return forecast\n",
    "        else:\n",
    "            print(\"Failed to fetch data:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function:\n",
    "weather_data = get_weather_data(\"1131200\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the necessary functions we can now try running our assistant by creating a thread and adding a message requesting a weather forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()\n",
    "\n",
    "#Define the location name\n",
    "location_name = \"Porto\"\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=f\"How is the weather today in {location_name}?\",\n",
    ")\n",
    "\n",
    "print(message.content[0].text.value + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initiate a run to yield a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread.id,\n",
    "        assistant_id = assistant.id\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == 'completed': \n",
    "    messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "    )\n",
    "    print(messages)\n",
    "else:\n",
    "    print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the run status returns a different value - `required_action`. This likely means that the assistant called a function and is waiting for a response.\n",
    "\n",
    "Let's check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a run is trying to call a tool and if so,  act accordingly\n",
    "for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "    if tool.function.name == \"get_weather_data\":\n",
    "\n",
    "        # let's start by checking the content of the tool call\n",
    "        print(tool.function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assistant wants to call the `get_weather_data` tool using the value \"Porto\" as the location argument.\n",
    "\n",
    "While the assistant is smart enough to realize that it should use the get_weather_data tool to answer the user's question it does not know how to proceed, because we never linked that tool with any of our two previously defined functions.\n",
    "\n",
    "The set of instructions to be performed when an assistant calls a function are defined in the runner. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check if a run is trying to call a tool and if so,  act accordingly\n",
    "for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "    if tool.function.name == \"get_weather_data\":\n",
    "\n",
    "        # First, call the get_location_id function to get the location ID for the location argument in the tool call\n",
    "        location_id = get_location_id(json.loads(tool.function.arguments)[\"location\"])\n",
    "\n",
    "        # Now, knowing the location ID, we can call the get_weather_data function to get the weather data for the location\n",
    "        forecast = get_weather_data(location_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast returns the following json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide the user with relevant weather data we shall return the minimum and maximum temperature as well as the rain probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rain probability: {forecast['precipitaProb']}, Max temperature: {forecast['tMin']}, Min temperature: {forecast['tMax']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output this data we first define an array in which to store the tool outputs. In this case we only have one single tool, but this would be especially useful if we were dealing with multiple tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array to store the tool outputs\n",
    "tool_outputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we submit the output of the `get_weather_data` function to the run by appending an \"output\" key to the output array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_outputs.append({\"tool_call_id\": tool.id,\n",
    "                    \"output\": f\"Rain probability: {forecast['precipitaProb']}, Max temperature: {forecast['tMin']}, Min temperature: {forecast['tMax']}\"\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the tool outputs we must now add them to the runner so that our assistant can use them to answer our initial question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tool_outputs:\n",
    "    try:\n",
    "        run = client.beta.threads.runs.submit_tool_outputs_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "            tool_outputs=tool_outputs\n",
    "        )\n",
    "        print(\"Tool outputs submitted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to submit tool outputs:\", e)\n",
    "else:\n",
    "    print(\"No tool outputs to submit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check for the run completion and display the output message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == 'completed':\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "    print(messages.data[0].content[0].text.value)\n",
    "else:\n",
    "    print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated runner with the tool call handling code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_run (thread_id, assistant_id, client):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread_id,\n",
    "        assistant_id = assistant_id,\n",
    "        instructions = \"Please address the user as SISTCA student.\"\n",
    "    )   \n",
    "\n",
    "    if run.status == 'completed': \n",
    "\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id = thread_id\n",
    "        )\n",
    "\n",
    "        # This code acts upon the presence of image files when using code interpreter\n",
    "        if messages.data[0].content[0].type == 'image_file':\n",
    "            return(messages.data[0].content[0].image_file.file_id)\n",
    "        else:\n",
    "            print(run.status)\n",
    "\n",
    "        return(messages.data[0].content[0].text.value)        \n",
    "    \n",
    "    # The following code is used to handle tool/function calls\n",
    "\n",
    "    # Preparing tool outputs\n",
    "    tool_outputs = []\n",
    "    for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "        if tool.function.name == \"get_weather_data\":\n",
    "            location_id = get_location_id(json.loads(tool.function.arguments)[\"location\"])\n",
    "            forecast = get_weather_data(location_id)\n",
    "            tool_outputs.append({\"tool_call_id\": tool.id,\n",
    "                                \"output\": f\"Rain probability: {forecast['precipitaProb']}, Max temperature: {forecast['tMin']}, Min temperature: {forecast['tMax']}\"\n",
    "                                })\n",
    "\n",
    "    # Submitting tool outputs and polling for completion status\n",
    "    if tool_outputs:\n",
    "        try:\n",
    "            run = client.beta.threads.runs.submit_tool_outputs_and_poll(\n",
    "                thread_id=thread_id,\n",
    "                run_id=run.id,\n",
    "                tool_outputs=tool_outputs\n",
    "            )\n",
    "            print(\"Tool outputs submitted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to submit tool outputs:\", e)\n",
    "    else:\n",
    "        print(\"No tool outputs to submit.\")\n",
    "\n",
    "    # Retrieving messages after tool output submission\n",
    "    if run.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "        print(messages.data[0].content[0].text.value)\n",
    "    else:\n",
    "        print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same logic we can also stream the response in the runner. However, since function calling uses different events that can conflict we the previously defined ones, instead of updating the streaming_run function we will create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_run_fc (thread_id, assistant_id, client):\n",
    "\n",
    "    class EventHandler(AssistantEventHandler):\n",
    "        @override\n",
    "        def on_event(self, event):\n",
    "\n",
    "            # Retrieve events that are denoted with 'requires_action'\n",
    "            # since these will have our tool_calls\n",
    "            \n",
    "            if event.event == 'thread.run.requires_action':\n",
    "                run_id = event.data.id  # Retrieve the run ID from the event data\n",
    "                self.handle_requires_action(event.data, run_id)\n",
    "    \n",
    "        def handle_requires_action(self, data, run_id):\n",
    "            tool_outputs = []\n",
    "            \n",
    "            for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
    "                if tool.function.name == \"get_weather_data\":\n",
    "                    location_id = get_location_id(json.loads(tool.function.arguments)[\"location\"])\n",
    "                    forecast = get_weather_data(location_id)\n",
    "                    tool_outputs.append({\"tool_call_id\": tool.id,\n",
    "                                        \"output\": f\"Rain probability: {forecast['precipitaProb']}, Max temperature: {forecast['tMin']}, Min temperature: {forecast['tMax']}\"\n",
    "                                        })\n",
    "            \n",
    "            # Submit all tool_outputs at the same time\n",
    "            self.submit_tool_outputs(tool_outputs, run_id)\n",
    "    \n",
    "        def submit_tool_outputs(self, tool_outputs, run_id):\n",
    "            # Use the submit_tool_outputs_stream helper\n",
    "            with client.beta.threads.runs.submit_tool_outputs_stream(\n",
    "                thread_id=self.current_run.thread_id,\n",
    "                run_id=self.current_run.id,\n",
    "                tool_outputs=tool_outputs,\n",
    "                event_handler=EventHandler(),\n",
    "            ) as stream:\n",
    "                for text in stream.text_deltas:\n",
    "                    print(text, end=\"\", flush=True)\n",
    "                print()\n",
    "    \n",
    "    \n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id,\n",
    "        event_handler=EventHandler()\n",
    "    ) as stream:\n",
    "        stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Code Interpreter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assistant's Code Interpreter makes it possible for assistants to write and run python code to answer user questions and analyze files.\n",
    "\n",
    "The Code Interpreter works similarly to the tools in the previous tutorial as it too is a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will attempt to portray Code Interpreter's usefulness for data analysis by providing it with a file containing the data of 100 customers of a ficticious organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"Tutorials/Assistants API/resources/customers-100.csv\", \"rb\"),\n",
    "    purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use code interpreter it must be passsed as a tool parameter of the assistant object. We will also be adding our previously defined file as a resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"Your purpose is to analyze the provided documents and to provide answers based on them.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    tool_resources={\n",
    "        \"code_interpreter\": {\n",
    "        \"file_ids\": [file.id]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a thread and will try asking the assistant for the customer ID of Sheryl Baxter, the customer in the first row of the provided file. If it works, the value `DD37Cf93aecA6Dc` should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What is the customer Id for Sheryl Baxter?\"\n",
    ")\n",
    "\n",
    "print(message.content[0].text.value + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time will simply call our previously defined runner instead of writing the runner code all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_run(thread.id, assistant.id, client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! \n",
    "\n",
    "Now, for something more complex, let's ask the assistant to create a graph using the provided file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Create a graph of the number of customers per country.\"\n",
    ")\n",
    "\n",
    "print(message.content[0].text.value + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action should return a file with the attached graph. When an assistant return a file it is marked as an `image_file`. To access it we must get its id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be achieved by checking for the presence of an `image_file` in the return messages of a completed run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == 'completed': \n",
    "\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id = thread.id\n",
    "    )\n",
    "\n",
    "    # This code acts upon the presence of image files when using code interpreter\n",
    "    if messages.data[0].content[0].type == 'image_file':\n",
    "        print(messages.data[0].content[0].image_file.file_id)\n",
    "    else:\n",
    "        print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated runner with this code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_run (thread_id, assistant_id, client):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread_id,\n",
    "        assistant_id = assistant_id,\n",
    "        instructions = \"Please address the user as SISTCA student.\"\n",
    "    )   \n",
    "\n",
    "    if run.status == 'completed': \n",
    "\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id = thread_id\n",
    "        )\n",
    "\n",
    "        # This code acts upon the presence of image files when using code interpreter\n",
    "        if messages.data[0].content[0].type == 'image_file':\n",
    "            return(messages.data[0].content[0].image_file.file_id)\n",
    "        else:\n",
    "            print(run.status)\n",
    "\n",
    "        return(messages.data[0].content[0].text.value)        \n",
    "    \n",
    "    # The following code is used to handle tool/function calls\n",
    "\n",
    "    # Preparing tool outputs\n",
    "    tool_outputs = []\n",
    "    for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "        if tool.function.name == \"get_weather_data\":\n",
    "            location_id = get_location_id(json.loads(tool.function.arguments)[\"location\"])\n",
    "            forecast = get_weather_data(location_id)\n",
    "            tool_outputs.append({\"tool_call_id\": tool.id,\n",
    "                                \"output\": f\"Rain probability: {forecast['precipitaProb']}, Max temperature: {forecast['tMin']}, Min temperature: {forecast['tMax']}\"\n",
    "                                })\n",
    "\n",
    "    # Submitting tool outputs and polling for completion status\n",
    "    if tool_outputs:\n",
    "        try:\n",
    "            run = client.beta.threads.runs.submit_tool_outputs_and_poll(\n",
    "                thread_id=thread_id,\n",
    "                run_id=run.id,\n",
    "                tool_outputs=tool_outputs\n",
    "            )\n",
    "            print(\"Tool outputs submitted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to submit tool outputs:\", e)\n",
    "    else:\n",
    "        print(\"No tool outputs to submit.\")\n",
    "\n",
    "    # Retrieving messages after tool output submission\n",
    "    if run.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "        print(messages.data[0].content[0].text.value)\n",
    "    else:\n",
    "        print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the file ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = std_run(thread.id, assistant.id, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the file we must call the `files endpoint` using the `file_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = client.files.content(file_id=file_id)\n",
    "image_data_bytes = image_data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and display the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython, os\n",
    "\n",
    "image_name = \"graph.png\"\n",
    "\n",
    "image_path = os.path.join(\"Tutorials/Assistants API/resources\", image_name)  # Path to save the image\n",
    "\n",
    "with open(image_path, \"wb\") as image_file:\n",
    "    image_file.write(image_data_bytes)\n",
    "\n",
    "IPython.display.Image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 File Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "File search is a tool that complements the Assistants API. It allows users to send their documents and question the assistant about them. The documents sent by the user are transformed in embeddings and stored in vector stores. File search uses both keyword and vector search to retrieve relevant information.\n",
    "\n",
    "\n",
    "This feature implements good retrieval pratices to extract the necessary data to answer the user query:\n",
    "* Rewrites user queries to optimise them for search.\n",
    "* Breaks down complex user queries into multiple searches that can run in parallel.\n",
    "* Runs both keyword and semantic searches across both assistant and thread vector stores.\n",
    "* Reranks search results to pick the most relevant ones before generating the final response.\n",
    "\n",
    "\n",
    "File search supports a variety of different file formats. For this tutorial, a simple txt containing some wikipedia articles was the source of the information.\n",
    "\n",
    "* .c\t\n",
    "* .cs\t\n",
    "* .cpp\t\n",
    "* .doc\t\n",
    "* .docx\t\n",
    "* .html\t\n",
    "* .java\t\n",
    "* .json\t\n",
    "* .md\t\n",
    "* .pdf\t\n",
    "* .php\t\n",
    "* .pptx\t\n",
    "* .py\t\n",
    "* .rb\t\n",
    "* .tex\t\n",
    "* .txt\t\n",
    "* .css\t\n",
    "* .js\t\n",
    "* .sh\t\n",
    "* .ts\t\n",
    "\n",
    "**Vector Store**\n",
    "\n",
    "A vector store is a digital storage system that holds vectors. Adding a file to a vector store automatically parses, chunks, embeds and stores the file in a vector database that is capable of both keyword and semantic search. Adding a file or files to a vector store is an async operation. The maximum file size you can store 512 MB and each file can not contain more that 5,000,000 tokens, this is computed automatically when a file is attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to send txt file with some wikipedia articles about artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Music Information Assistant\",\n",
    "  instructions=\"You are an expert music analyst. Use your knowledge base to answer questions regarding musical artists and their work.\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector store to store your information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Music Information\")\n",
    "\n",
    "# Add the desired files. It is possible to add multiple files\n",
    "file_paths = [\"Tutorials/Assistants API/resources/file_search_information.txt\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    " \n",
    "# upload the files and add them to the vector store\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, update the assistant with the newly created vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n",
    "# upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"Tutorials/Assistants API/resources/file_search_information.txt\", \"rb\"), purpose=\"assistants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a thread and ask the assistant a question whilst attaching the file to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"How many artists do you know about, and what are their albums?\",\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, run the thread and print the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_run(thread.id, assistant.id, client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Embeddings\n",
    "\n",
    "OpenAI’s text embeddings transforms strings to numbers, that allows to measure the relatedness of text strings. Embeddings are commonly used for:\n",
    "\n",
    "* Search (where results are ranked by relevance to a query string)\n",
    "* Clustering (where text strings are grouped by similarity)\n",
    "* Recommendations (where items with related text strings are recommended)\n",
    "* Anomaly detection (where outliers with little relatedness are identified)\n",
    "* Diversity measurement (where similarity distributions are analyzed)\n",
    "* Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "\n",
    "\n",
    "Like mentioned before, Embeddings have a lot of uses but this tutorial will only focus on how to make the request and show simple method to compare two strings.\n",
    "\n",
    "**Models**\n",
    "\n",
    "Right now there are three available models for Embeddings. The ones with \"-3\" on the name are third generation models.\n",
    "\n",
    "* text-embedding-3-small\t\n",
    "* text-embedding-3-large\n",
    "* text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve the embeddings**\n",
    "\n",
    "*model* - changes the model use to retieve information  \n",
    "*input* - string message you want to retrieve the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = client.embeddings.create(\n",
    "    input=\"We are testing to see if this string has any similarities to another one.\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embeddings1 = response1.data[0].embedding\n",
    "\n",
    "print(f\"Embeddings Example - {embeddings1[0]}, {embeddings1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare two strings using embeddings and cosine similarity\n",
    "\n",
    "**1 - Import numpy and cosine_similarity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Retrieve the embeddings for the second string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = client.embeddings.create(\n",
    "    input=\"We're experimenting to determine if this string bears resemblance to another.\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embeddings2 = response2.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Convert the embeddings to numpy arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = np.array(embeddings1).reshape(1, -1)\n",
    "embeddings2 = np.array(embeddings2).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 - Calculate the similarity**\n",
    "\n",
    "The closer the score is to 1 the similar it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "final_score = float(format(similarity_score[0][0], \".2f\"))\n",
    "\n",
    "print(f\"Similarity: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4  Vision\n",
    "\n",
    "Vision API from OpenAI, based on GPT-4 Turbo, is an enormous jump in AI capability. This functionality brought AI the capability to take pictures as input, comprehend them and answer questions about them. This development guarantees to revolutionize the way we interact and use AI. \n",
    "\n",
    "Vision can accept images through links or even by passing its base64 encoded image. Unfortunately, this feature is only available to ChatGPT-4, so we will be addressing this tutorial at the end of our article as an extra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the `response` is structered, passing the role, content and the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Image Generation (DALL-E)\n",
    "\n",
    "\n",
    "\n",
    "The open AI image API provides three methods for interacting with images, namely creating images from scratch using a text prompt (DALL-E 3 and DALL-E 2), creating edited versions of images by having the model change some areas of a pre-existing image based on a new text prompt (DALL-E 2 only) and creating variations of an existing image (DALL-E 2 only).\n",
    "\n",
    "This guide covers the basics of the API methods with useful code examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Image Generation \n",
    "\n",
    "The image generation method allows you to create an original image with a text prompt.\n",
    "\n",
    "Initially, it is necessary to import the display and Image functions from the IPython.display module, in order to display the image inside a Jupyter notebook.\n",
    "\n",
    "It also imports os, used for operating system operations, as explained earlier for creating a venv environment, and imports the OpenAI class from the openai module, an interface for using the API.\n",
    "\n",
    "The code creates an instance of the OpenAI() client.\n",
    "A request is made to the API to generate an image, with the parameters:\n",
    "\n",
    "* __model :__ specifies the model used to generate the image, in this case \"dall-e-2\", which specialises in generating images based on textual descriptions;\n",
    "* __prompt :__ descriptive text that will serve as input for the model to generate the image, in this example, \"A cat inside a car\";\n",
    "* __size :__ specifies the size, 1024x1024 pixels;\n",
    "* __quality :__  sets the quality, \"standard\". When using DALL-E 3 it is possible to set quality: \"hd\", i.e. fine detail. However, standard quality square images are generated more quickly;\n",
    "* __n :__ defines the number of images generated.\n",
    "\n",
    "After the API generates the image, the image URL is extracted from the response and stored in the image_url variable.\n",
    "Finally, the image URL is printed and the image is displayed using the display function with the Image class, passing the URL as a parameter and setting the image width to 500 pixels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.generate(\n",
    "  model=\"dall-e-2\",\n",
    "  prompt=\"A cat inside a car\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "print(image_url)\n",
    "display(Image(url=image_url, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Edits (DALLE 2 only)\n",
    "\n",
    "Also known as \"inpainting\", the image editing endpoint allows you to edit an image by loading an image and a mask indicating the areas that should be replaced, you can use tools such as [GIMP](https://www.gimp.org/) or [Photoshop](https://www.adobe.com/pt/products/photoshop/landpa.html?gclid=CjwKCAjw57exBhAsEiwAaIxaZrgKDCM6FAEGOCavdUMdwMkJnL6o9cWGXjdYdjYN1DoN5HeWLj6-nBoCTjgQAvD_BwE&mv=search&s_kwcid=AL!3085!3!340859421278!e!!g!!adobe%20photoshop!1447265685!53212492301&mv=search&mv2=paidsearch&sdid=2XBSBWBF&ef_id=CjwKCAjw57exBhAsEiwAaIxaZrgKDCM6FAEGOCavdUMdwMkJnL6o9cWGXjdYdjYN1DoN5HeWLj6-nBoCTjgQAvD_BwE:G:s&s_kwcid=AL!3085!3!340859421278!e!!g!!adobe%20photoshop!1447265685!53212492301&gad_source=1), to erase a certain area of the image. \n",
    "The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the complete new image, not just the deleted area.\n",
    "\n",
    "The image and mask sent must be square PNG images of less than 4 MB and must also have the same dimensions. The non-transparent areas of the mask are not used to generate the output, so they don't necessarily have to match the original image as in the following example, which shows the original image, the mask and the resulting image after the editing process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"pt\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Imagens</title>\n",
    "<style>\n",
    "    figure {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Espaçamento entre as imagens */\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/original.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Original</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/mask.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Mask</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/output.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Output</figcaption>\n",
    "</figure>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.edit(\n",
    "  model=\"dall-e-2\",\n",
    "  image=open(\"Tutorials/DALLE/original.png\", \"rb\"),\n",
    "  mask=open(\"Tutorials/DALLE/mask.png\", \"rb\"),\n",
    "  prompt=\"A hand holding a sandwich\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "\n",
    "print(image_url)\n",
    "display(Image(url=image_url, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Variations (DALL·E 2 only)\n",
    "\n",
    "The image variations endpoint allows you to generate a variation of a given image.\n",
    "\n",
    "Similar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.create_variation(\n",
    "  model=\"dall-e-2\",\n",
    "  image=open(\"Tutorials/DALLE/original.png\", \"rb\"),\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "\n",
    "print(image_url)\n",
    "display(Image(url=image_url, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images below correspond to a possible example of the endpoint variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"pt\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Imagens</title>\n",
    "<style>\n",
    "    figure {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Espaçamento entre as imagens */\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/original.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Original</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/variation.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Variation</figcaption>\n",
    "</figure>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 TTS Tutorial\n",
    "\n",
    "In this tutorial you will learn how to integrate text-to-speech feature from open-ai in your projects.\n",
    "\n",
    "| Available voices | Output Formats           |\n",
    "|------------------|--------------------------|\n",
    "| Alloy            | **mp3** - default format|\n",
    "| Echo             | **opus**                 |\n",
    "| Fable            | **aac**                  |\n",
    "| Onyx             | **flac**                 |\n",
    "| Nova             | **waw**                  |\n",
    "| Shimmer          | **pcm**                  |\n",
    "\n",
    "\n",
    "Voice only changes the tone and the \"person\" who is speaking. TTS only produces english audio files.\n",
    "\n",
    "\n",
    "There are two models available tts right now: **tts-1** and **tts-1-hd**. If you want lower latency **tts-1** is recommended, but it comes with lower quality than **tts-1-hd**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Importing the necessary libraries**\n",
    "\n",
    "pathlib - offers classes that represent a filesystem path  \n",
    "IPython - serves to display the audio in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Create a path to save the audio file**\n",
    "\n",
    "In here you can choose the format you want your audio file to be in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file_path = Path(f\"tts_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Generate the audio**\n",
    "\n",
    "The endpoint will receive a model, a voice and your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"shimmer\",\n",
    "  input=\"Hey, I'm a student in Lincenciatura de Engenharia de Telecomunicações e Informática in Instituto Superior de Engenharia do Porto, and I'm doing a tutorial on how to use open-ai in my projects!\"\n",
    ")\n",
    "\n",
    "response.write_to_file(speech_file_path)\n",
    "\n",
    "IPython.display.Audio(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will learn how to use Whisper to transcribe text from audio files as well translate it into English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the audio file**\n",
    "\n",
    "Feel free to try different audio files as well as add/record your own. Files must be of one of these types: mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n",
    "\n",
    "**Note that files greater than 25MB will need to be segmented using [additional libraries](https://platform.openai.com/docs/guides/speech-to-text/longer-inputs).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"Tutorials/Whisper/audio.wav\", \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Transcribe an audio file**\n",
    "\n",
    "The transcription endpoint will take the input audio and transcribe it into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    ")\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how **response_format=\\\"text\\\"**? To get additional information to get additional information try changing it to **verbose_json**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now receive a json response with additional parameters. One of which, the **language** parameter, includes the detected language from the input file.\n",
    "\n",
    "**Note:** If the language is not being properly detected, which may negatively impact transcription, you can add an additional parameter stating it according to the  [ISO-639-1 format](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n",
    "\n",
    "``` json\n",
    "(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    "    language=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "We can modify our code to reflect this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"verbose_json\"\n",
    ")\n",
    "print(f\"Detected language: {transcription.language}\")\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Translation**\n",
    "\n",
    "Using the translation endpoint we can translate the contents of the audio file to English (currently this the only available language for translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = client.audio.translations.create(\n",
    "    model = \"whisper-1\", \n",
    "    file = audio_file,\n",
    "    response_format=\"text\"\n",
    ")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moderation endpoint is a tool you can use to check whether text is potentially harmful. It can be used to identify content that could be harmful and take action.\n",
    "\n",
    "The templates classify the following categories:\n",
    "* `Hate` - Content that expresses or promotes hatred based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status or caste. Hateful content directed at unprotected groups constitutes harassment.\n",
    "* `Hate/Threatening` - Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.\n",
    "* `Harassment` - Content that expresses, incites, or promotes harassing language towards any target.\n",
    "* `Harassment/Threatening` - Harassment content that also includes violence or serious harm towards any target.\n",
    "* `Self-harm` - Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.\n",
    "* `Self-harm/Intent` - Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.\n",
    "* `Self-harm/Instructions` - Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.\n",
    "* `Sexual` - Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).\n",
    "* `Sexual/Minors` - Sexual content that includes an individual who is under 18 years old.\n",
    "* `Violence` - Content that depicts death, violence, or physical injury.\n",
    "* `Violence/Graphic` - Content that depicts death, violence, or physical injury in graphic detail.\n",
    "\n",
    "\n",
    "To obtain a classification for a piece of text, a request is made to the moderation endpoint, as shown in the following code fragment.\n",
    "Firstly, the OpenAI class from the openai module is imported, then an instance of the class is created and assigned to a variable, configuring the client to interact with the OpenAI API.\n",
    "\n",
    "With the `moderations.create()` method, the text is sent for moderation.\n",
    "\n",
    "The response from the API will be a JSON object with information about the moderation of the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.moderations.create(\n",
    "    input=\"I own a lot of guns which I intend to use, in order to harm people\"\n",
    ")\n",
    "\n",
    "output = response.results[0]\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the endpoint response structure. It returns the following fields:\n",
    "\n",
    "* `flagged`: Set to true if the model classifies the content as potentially harmful, false otherwise.\n",
    "\n",
    "* `categories`: Contains a dictionary of violation flags for each category. The value will be true if the model flags the corresponding category as violated, false otherwise.\n",
    "* `category_scores`: Contains a dictionary of raw scores per category issued by the model, denoting the model's confidence that the input violates the OpenAI policy for the category. The value is between 0 and 1, with higher values denoting greater confidence. The scores should not be interpreted as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` json\n",
    "{\n",
    "    \n",
    "    \"id\": \"modr-XXXXX\",\n",
    "    \"model\": \"text-moderation-007\",\n",
    "    \"results\": [\n",
    "        {\n",
    "            \"flagged\": true,\n",
    "            \"categories\": {\n",
    "                \"sexual\": false,\n",
    "                \"hate\": false,\n",
    "                \"harassment\": false,\n",
    "                \"self-harm\": false,\n",
    "                \"sexual/minors\": false,\n",
    "                \"hate/threatening\": false,\n",
    "                \"violence/graphic\": false,\n",
    "                \"self-harm/intent\": false,\n",
    "                \"self-harm/instructions\": false,\n",
    "                \"harassment/threatening\": true,\n",
    "                \"violence\": true\n",
    "            },\n",
    "            \"category_scores\": {\n",
    "                \"sexual\": 1.2282071e-6,\n",
    "                \"hate\": 0.010696256,\n",
    "                \"harassment\": 0.29842457,\n",
    "                \"self-harm\": 1.5236925e-8,\n",
    "                \"sexual/minors\": 5.7246268e-8,\n",
    "                \"hate/threatening\": 0.0060676364,\n",
    "                \"violence/graphic\": 4.435014e-6,\n",
    "                \"self-harm/intent\": 8.098441e-10,\n",
    "                \"self-harm/instructions\": 2.8498655e-11,\n",
    "                \"harassment/threatening\": 0.63055265,\n",
    "                \"violence\": 0.99011886\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Exercise A\n",
    "\n",
    "Embeddings have a lot of uses, when combined with other APIs can do even more. One example is using embeddings with chat completion to extract information from a pdf and then create a function to ask anything about the document.\n",
    "\n",
    "In the following exercise you will create a program that retrieves information from a pdf and answer questions about it. In order to achieve this you must:\n",
    "*   Convert a pdf file to embeddings and save them in a csv file \n",
    "*   Use embeddings to search a user query in the csv file\n",
    "*   Send that information to chat completions\n",
    "\n",
    "The pdf used in this exercise will be `LETI_SISTCA_2023_24_Team2_OpenAI.pdf`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start by importing the requiring dependecies and initialize the client and creating constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd  \n",
    "import re \n",
    "import tiktoken \n",
    "import PyPDF2\n",
    "\n",
    "import ast\n",
    "from scipy import spatial  \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"Contents\",\n",
    "    \"List of Tables\",\n",
    "    \"List of Figures\",\n",
    "    \"References\",\n",
    "]\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "BATCH_SIZE = 1000   \n",
    "\n",
    "# TODO #1: Create consts for GPT_MODEL and EMBEDDING_MODEL (small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Logic to Extract the information from the pdf**\n",
    "\n",
    "This is a simple logic to extract the necessary information from the pdf we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def split_sections_from_pdf(pdf_text):\n",
    "    title = []\n",
    "    text = []\n",
    "    ignore = True\n",
    "    current_section = \"I.N.I.T.I.A.L-V.A.L.U.E\"\n",
    "    for line in pdf_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        if is_new_section(line):\n",
    "            ignore = ignore_section(line)\n",
    "            if ignore:\n",
    "                continue\n",
    "            title.append(line)\n",
    "            if current_section != \"I.N.I.T.I.A.L-V.A.L.U.E\":\n",
    "                text.append(current_section)\n",
    "            current_section = \"\"\n",
    "        else:\n",
    "            if not ignore:\n",
    "                current_section += \" \" + line\n",
    "    if current_section:\n",
    "        text.append(current_section)\n",
    "        \n",
    "    \n",
    "    sections = [(title),(text)]\n",
    "    return sections\n",
    "\n",
    "def ignore_section(line):\n",
    "    if any(section in line for section in SECTIONS_TO_IGNORE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_new_section(line):\n",
    "    pattern = r\"\\d+\\.\\d+(?:\\.\\d+)? [A-Z].*?\"\n",
    "    \n",
    "    if line.strip().count('.') > 7:\n",
    "        return False\n",
    "    if re.match(pattern, line.strip()):\n",
    "        return True \n",
    "    if any(section in line for section in SECTIONS_TO_IGNORE):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def clean_section(section):\n",
    "\n",
    "    titles = section[0]\n",
    "    text = section[1]\n",
    "    \n",
    "    for line in text:\n",
    "        line = re.sub(r\"\\[\\d+\\]\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\d\\d+\\]\", \"\", line)\n",
    "        line = line.strip()\n",
    "\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "def num_tokens(text, model = GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def halved_by_delimiter(string, delimiter = \"\\n\"):\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  \n",
    "    elif len(chunks) == 2:\n",
    "        return chunks \n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "    \n",
    "    \n",
    "def truncated_string(string, model, max_tokens, print_warning = True,):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(title, text, max_tokens = 1000, model = GPT_MODEL, max_recursion = 5):\n",
    "\n",
    "    string = \"\\n\\n\".join(title + text)\n",
    "    num_tokens_in_string = num_tokens(string, model)\n",
    "\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model, max_tokens)]\n",
    "\n",
    "    else:\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_strings = split_strings_from_subsection(title, half,max_tokens,model,max_recursion - 1)\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "            \n",
    "    return [truncated_string(string, model, max_tokens)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call all functions to retrieve the clean pdf sections to then split it into strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO #2: Call the previous created functions\n",
    "pdf_text = extract_text_from_pdf(\"LETI_SISTCA_2023_24_Team2_OpenAI.pdf\")\n",
    "pdf_sections = split_sections_from_pdf(pdf_text)\n",
    "cleaned_sections = clean_section(pdf_sections)\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "strings = []\n",
    "titles = cleaned_sections[0]\n",
    "texts = cleaned_sections[1]\n",
    "for i in range(len(titles)):\n",
    "    strings.extend(split_strings_from_subsection(titles[i], texts[i], max_tokens=MAX_TOKENS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming the information to embeddings and saving it to a CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for batch_start in range(0, len(strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = strings[batch_start:batch_end]\n",
    "    \n",
    "    # TODO #3: Make a request to the embeddings API with the batch as input\n",
    "    \n",
    "    \n",
    "    for i, be in enumerate(response.data):\n",
    "        assert i == be.index  \n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame({\"text\": strings, \"embedding\": embeddings})\n",
    "\n",
    "\n",
    "SAVE_PATH = \"SISTCA_TEAM2.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is done, now we need to create a function so GPT can awnser anything about the pdf using the saved embeddings**\n",
    "\n",
    "**Change the Embedding Model and read the CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO #4: Change the EMBEDDING_MODEL (ada)\n",
    "\n",
    "\n",
    "# TODO #5: Create a variable with the CSV file path\n",
    "\n",
    "\n",
    "df = pd.read_csv(embeddings_path)\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to compare the relatedness off the strings with the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(query, df , relatedness_fn = lambda x, y: 1 - spatial.distance.cosine(x, y), top_n = 100) :\n",
    "    \n",
    "    # TODO 6: Make a request to the embeddings API with the query as input\n",
    "\n",
    "    \n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n",
    "\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"open ai\", df, top_n=5)\n",
    "\n",
    "\n",
    "def num_tokens(text, model = GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def query_message(query,df, model, token_budget):\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below articles on the document about OpenAI made by Team 2, composed by Patrícia Sousa, Carlos Alves, Jose Leal and Tiago Ribeiro, for SISTCA to answer the subsequent question. If the answer cannot be found in the articles, write \"Sorry, the information you seek cannot be found in the document in question.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nPDF article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the final function**  \n",
    "\n",
    "\n",
    "If the chat completions does not have information to answer the query, like defined, it will say _\"Sorry, the information you seek cannot be found in the document in question.\"_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, df = df, model = GPT_MODEL, token_budget = 4096 - 500, print_message = False):\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about the document made by Team2 for SISTCA about OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    \n",
    "    # TODO 7: Make a request to the Chat Completions API\n",
    "    \n",
    "\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the ask function to verify it**\n",
    "\n",
    "Because everytime you run ask you give a new prompt to chat completions the awnsers may very for one attempt to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the function: Be aware that the awnsers are not the same everytime you do the request\n",
    "print(ask(\"Scientific/technological background\")) \n",
    "print(ask(\"Give me the authors\")) \n",
    "print(ask(\"What can you tell me about the document\")) \n",
    "print(ask(\"Give me the document structure\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Exercise B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite, in it's current state, only allowing English translation, we can increment the Whisper API with other APIs so as to be able to translate audio into other languages.\n",
    "By using the standard GPT model in the Chat Completions API we can translate text to and from any language.\n",
    "\n",
    "In the following exercise you will create a program that translates an audio message into any other language. In order to achieve this you must:\n",
    "\n",
    " - Use Whisper to transcribe the original audio;\n",
    " - Translate the resulting transcription into any language using Chat Completions;\n",
    " - Return the translated audio using Text-To-Speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start by importing the required dependencies and initializing the client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Whisper to transcribe the original audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #1: load the audio file\n",
    "\n",
    "\n",
    "\n",
    "# TODO #2: transcribe contents and detect the input language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate the transcribed text into any language using Chat Completions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #3: translate the transcription to another language using chatCompletions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the translated audio using Text-To-Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #4: output the translated audio file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the output audio file\n",
    "output_file_path = Path(__file__).parent / f\"translation.mp3\"\n",
    "translated_audio.write_to_file(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the output\n",
    "import IPython.display as display\n",
    "\n",
    "print(f\"Detected language: {detected_language} \\n\")\n",
    "print(f\"Transcribed text: {transcribed_text} \\n\")\n",
    "\n",
    "print(f\"Translated text: {translated_text} \\n\")\n",
    "print(f\"Translated audio file saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "display.Audio(speech_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Challenge\n",
    "\n",
    "With all tutorials done, you should now be able to answer correctly to this final challenge. \n",
    "In this section we propose you the following exercise: You have to create an assistant which will be capable of analysing and answering questions about films, series and books. The average question should look like this, 'Give me the top 10 best films of all time.'. \n",
    "\n",
    "\n",
    "We advise that you follow this steps:\n",
    "\n",
    "- Create an Assistant with function calling.\n",
    "  - For better flexibility, you should retrieve from the user query the type (movie, series, book), theme (comedy, thriller...), and quantity.\n",
    "- Create a function to retrieve information from Chat Completions.\n",
    "  - You can force Chat Completions to send the information in a specific format, like JSON.\n",
    "- Show the response in a bullet list.\n",
    "\n",
    "\n",
    "In our [github](https://github.com/carlosgalvesisep/SISTCA-OpenAI) you can find a Flask template where you can visually test the results of your challenge. To have a compatibility with the template, make sure that your final function, the one that retrieves the assistant message, follows this format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cinema_info(type_filter, genre_filter, quantity_filter):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Future Work\n",
    "\n",
    "The OpenAI API is still an in development effort this means that everything is prone to change.\n",
    "In fact, some of it has changed during the development of this script. As previously noted before\n",
    "some of the functionalities that have been addressed is not yet available for the explorer tier of the\n",
    "API, which includes the \"GPT-3.5-Turbo\" model. At the time of writing this, the launch of a new\n",
    "flagship model is on the horizon - \"GPT-4o\" [10]. This should make previously tier-locked APIs\n",
    "such as Vision accessible to everyone, as well as potentially added new features.  \n",
    "\n",
    "Despite the rather wide scope of this script some of OpenAI’s tools haven’t been explored due\n",
    "to them being unavailable in the API. Such is the case for Sora [11], a video generation model, or\n",
    "the research publications like CLIP [12, 13], Jukebox [14, 15] or Point-E [16, 17]. Another feature\n",
    "unable to be explored, despite being compatible with \"GPT-3.5-Turbo\", was fine-tuning. This\n",
    "feature allows to train \"GPT\" with your own dataset [18].  \n",
    "\n",
    "For a future update of this script or development of a new one with the same topic you can\n",
    "explored this features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] IBM, “What is an ai model?,” IBM. Accessed: 2024-03-28.\n",
    "\n",
    "[2] R. Merritt, “What is a transformer model?,” NVIDIA Blog, mar 2022. (Accessed: 2024-03-28.\n",
    "\n",
    "[3] “Openai.” Available at: https://openai.com/. Accessed: 2024-03-27.\n",
    "\n",
    "[4] xAI, “xai grok.” Available at: https://grok.x.ai/. (Accessed: 2024-03-27).\n",
    "\n",
    "[5] Anthropic, “Anthropic home.” Available at: https://www.anthropic.com/. (Accessed: 2024-\n",
    "03-27).\n",
    "\n",
    "[6] Google, “Deepmind technologies.” Available at: https://deepmind.google/technologies/. (Ac-\n",
    "cessed: 2024-03-27).\n",
    "\n",
    "[7] Cohere, “Cohere home.” Available at: https://cohere.com/. (Accessed: 2024-03-27).\n",
    "\n",
    "[8] “Python.” Available at: https://wiki.python.org/. (Accessed: 2024-03-29).\n",
    "\n",
    "[9] “Jupyter.” Available at: https://jupyter.org/. (Accessed: 2024-05-18).\n",
    "\n",
    "[10] “Hello gpt-4o.” Available at: https://openai.com/index/hello-gpt-4o/. (Accessed: 2024-05-18).\n",
    "\n",
    "[11] “Sora.” Available at: https://openai.com/index/sora/. (Accessed: 2024-05-18).\n",
    "\n",
    "[12] “Clip: Connecting text and images.” Available at: https://openai.com/index/clip/. (Accessed:\n",
    "2024-05-18).\n",
    "\n",
    "[13] “Clip on github.” Available at: https://github.com/openai/CLIP. (Accessed: 2024-05-18).\n",
    "\n",
    "[14] “Jukebox sample explorer.” Available at: https://jukebox.openai.com/. (Accessed: 2024-05-\n",
    "18).\n",
    "\n",
    "[15] “Jukebox on github.” Available at: https://github.com/openai/jukebox. (Accessed: 2024-05-\n",
    "18).\n",
    "\n",
    "[16] “Point-e: A system for generating 3d point clouds from complex prompts.” Available at:\n",
    "https://openai.com/index/point-e/. (Accessed: 2024-05-18).\n",
    "\n",
    "[17] “Point-e on github.” Available at: https://github.com/openai/point-e. (Accessed: 2024-05-18).\n",
    "\n",
    "[18] “Fine-tuning.” https://platform.openai.com/docs/guides/fine-tuning. [Online; accessed 2024-\n",
    "05-18].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
