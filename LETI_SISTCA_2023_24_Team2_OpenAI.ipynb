{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"left\">\n",
    "    <h1>SISTEMAS COMPUTACIONAIS AVANÇADOS (SISTCA) ADVANCED COMPUTING SYSTEMS</h1>\n",
    "    <h2>OpenAI</h2>\n",
    "    <h4>Degree in Telecommunications and Informatics Engineering </h4>\n",
    "    <h5>\n",
    "    <img width=\"500\" src=\"imgs/ISEP_logo.png\"><br>\n",
    "    2023 / 24</h5>\n",
    "</div>\n",
    "\n",
    "<table align=\"left\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Version</th>\n",
    "            <th>Date</th>\n",
    "            <th>Authors</th>\n",
    "            <th>Update information</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>V1.0</td>\n",
    "            <td>April 2024</td>\n",
    "            <td style=\"text-align: left;\">\n",
    "                Patrícia Sousa (1210713), Carlos Alves (1211604), José Leal (1211066), Tiago Ribeiro (1210924)\n",
    "                <br><br>\n",
    "                <strong>Supervisor:</strong> Paula Viana (pmv)\n",
    "            </td>\n",
    "            <td>Original version</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Context\n",
    "\n",
    "In this script we will present an API developed by openAI, one of the leading organisations in\n",
    "artificial intelligence (AI) research and development. Launched in 2020, it represents a significant\n",
    "milestone in accessing advanced AI technology. It offers an accessible and simplified interface for\n",
    "developers to integrate AI capabilities into a variety of applications, from text analysis to content\n",
    "generation.  \n",
    "\n",
    "The OpenAI API is based on recent advances in machine learning, in particular, deep neural\n",
    "network architectures, such as GPT (Generative Pre-trained Transformer) language models. These\n",
    "models have revolutionised the way machines understand and generate natural language, enabling\n",
    "tasks such as automatic translation, text summarising, question answering, and content generation.\n",
    "GPT is trained with a vast amount of textual data from the internet, allowing it to capture the\n",
    "nuances and complexities of human language. Through the pre-training process, the model learns\n",
    "to represent knowledge in a general way, without the need for specific training for a particular task.\n",
    "\n",
    "The OpenAI API leverages these pre-trained language models, providing a simple and effec-\n",
    "tive interface for developers to take advantage of their capabilities. This means users can easily\n",
    "integrate advanced AI capabilities into their applications, without needing to understand complex\n",
    "implementation details or model training.  \n",
    "\n",
    "Additionally, it is designed with a focus on security and ethics, including measures to prevent\n",
    "malicious or harmful use of AI technology. This reflects OpenAI’s commitment to promoting\n",
    "the responsible development of artificial intelligence and ensuring that its benefits are extremely\n",
    "accessible and used for the good of society.  \n",
    "\n",
    "## 1.2 Motivation\n",
    "\n",
    "One of the main motivations for choosing this topic lies in the possibility of structured presenting\n",
    "a language (and a platform) accessible to all students, providing them with the opportunity to work\n",
    "and apply it in the future, as it is an increasingly used topic day to day. Whether in the SISTCA\n",
    "curricular unit or in other contexts, the OpenAI API offers a versatile and robust environment to\n",
    "explore the potential of artificial intelligence.  \n",
    "\n",
    "When choosing this topic, the need to familiarise yourself not only with the fundamental con-\n",
    "cepts of artificial intelligence, but also with the practical tools for its implementation, is recognised.  \n",
    "\n",
    "This required not only mastering theoretical principles, but also exploring accessible programming\n",
    "languages and platforms such as the OpenAI API.\n",
    "\n",
    "## 1.3 Objectives\n",
    "\n",
    "The main objective of this script is to explore the various features offered by the OpenAI API\n",
    "and learn how to integrate them, promoting a comprehensive and practical understanding of the\n",
    "platform’s capabilities, providing users with a solid foundation to explore and use the following\n",
    "available tools:\n",
    "\n",
    "- **Chat Completions**: To understand the generic implementation of a chat-based GPT assistant.\n",
    "- **Assistants API + Tools**: Create a program that provides users with media suggestions such as books, films, or TV shows.\n",
    "- **Embeddings**: Learn how to represent words, sentences, paragraphs, or entire documents in a continuous vector space.\n",
    "- **Vision**: Use Vision functionality to extract information from an image.\n",
    "- **Image Generation/DALL-E**: Learn how to generate AI-created images.\n",
    "- **TTS (Text to Speech)**: Turn strings into robotized speech using AI.\n",
    "- **Whisper**: Capture audio and translate it into different languages.\n",
    "- **Moderation**: Explore OpenAI's Moderation functionality to detect inappropriate content.\n",
    "\n",
    "\n",
    "In addition, two practical exercises will be developed to apply the acquired knowledge and a\n",
    "challenge will be proposed to test the reader’s understanding and ability to use the OpenAI API.\n",
    "\n",
    "## 1.4 Document Structure\n",
    "\n",
    "This tutorial is organised into six chapters with the following structure: Introduction, Theo-\n",
    "retical, Setup/Installation, Tutorial/Functionality, Exercises and a Challenge.\n",
    "In the Introduction, some concepts about the OpenAI API will be presented that will be\n",
    "deepened throughout the script.  \n",
    "\n",
    "The theoretical part, will introduce the State-of-the-art and detail the API’s Features.\n",
    "Then, the third chapter focuses on Setup/Installation, explaining how to create an open AI\n",
    "account and set up a development environment.  \n",
    "\n",
    "In the Tutorial/Functionality part, a detailed tutorial of the available features will be provided.\n",
    "After all these topics, two exercises will be developed and their corresponding resolution will\n",
    "be presented and a final challenge to test the knowledge acquired by the users.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2 Theoretical (scientific/technological background)\n",
    "\n",
    "Before diving into specific AI models it’s important to have a general understanding of how AI\n",
    "generally works.  \n",
    "\n",
    "Most AI solutions in natural language processing are based on broader Large language mod-\n",
    "els (LLMs) these are algorithms that have been trained on vast amounts of data with the purpose\n",
    "of understanding and generating human like text. LLMs make use of the transformer type of\n",
    "architecture, a deep learning technique that, in short, represents text via numerical representations\n",
    "known as tokens and gives them different weights so as to be able to contextualise words and find\n",
    "similarities [1].  \n",
    "\n",
    "One example of a transformer based model is the Generative pre-trained transformer\n",
    "(GPT), an AI model that has been pre-trained on large sets of data via the use of the transformer\n",
    "architecture for general purpose tasks. Furthermore, these models can be fine tuned to achieve\n",
    "greater performance in more specific tasks, such is the case of image generation models [2].  \n",
    "\n",
    "Lastly, in order to make use of these models it’s essential to understand the concept of prompts.\n",
    "That is the name given to the textual inputs given to the model. These inputs are then broken\n",
    "down into the aforementioned tokens via a process we call tokenization, which facilitates the use\n",
    "of the models language structure.\n",
    "\n",
    "## 2.1 State-of-the-art\n",
    "\n",
    "AI, Artificial Intelligence, refers to a simulation of human intelligence in machines programmed\n",
    "to mimic human cognitive processes and actions. The concept of AI is not new, it has been around\n",
    "since the mid-20th century, but in the last few years there have been significant advances for AI.\n",
    "With these advances, due to its potential, AI has become increasingly important across various\n",
    "industries such as healthcare, finance, manufacturing, education, amongst others. Nowadays, one\n",
    "of the most famous companies developing AI products is OpenAI, which we are going to base\n",
    "our work on. However, just like any other business, OpenAI has its competitors. Some of them\n",
    "currently only dispose of a chat bot, and even that, at the time, are not available in Portugal.\n",
    "Listed below are some companies in the AI business.\n",
    "\n",
    "| AI                                      | **OpenAI**                          | **X**     | **Anthropic**                 | **Deepmind**         | **Cohere**       |\n",
    "|----------------------------------------|-------------------------------------|-----------|-------------------------------|----------------------|------------------|\n",
    "| **ChatBot**                            | ChatGPT                             | Grok      | Claude                        | Gemini               | Coral            |\n",
    "| **API**                                | Yes                                 | No        | Yes                           | Yes                  | Yes              |\n",
    "| **Inputs**                             | Text, Audio, Image, Video          |           | Text                          | Text, Image          | Text             |\n",
    "| **Outputs**                            | Text, Audio, Image, Video          |           | Text                          | Text                 | Text             |\n",
    "| **ChatBot and API Availability in Portugal** | Both                                | None      | API Only                      | ChatBot Only         | Both             |\n",
    "\n",
    "\n",
    "## OpenAI 2.2 Features\n",
    "\n",
    "OpenAI’s API offers a vast array of cutting-edge AI models based on deep learning and natural\n",
    "language processing techniques. These models have been trained on vast datasets and fine tuned\n",
    "to fulfil a variety of tasks such as text and image generation, audio and text conversions, amongst\n",
    "other things.\n",
    "\n",
    "### 2.2.1 GPT\n",
    "\n",
    "The GPT (Generative Pre-trained Transformer) series is OpenAI’s main set of large language\n",
    "models. These models are trained to understand and generate natural language text based on\n",
    "contextual inputs so as to better communicate with humans. The most widespread version, GPT-\n",
    "3.5 currently serves as the model that powers the free version of ChatGPT. Its understanding\n",
    "of human language allows for coherent conversations which makes it a suitable chat bot. GPT-4\n",
    "improves upon its predecessor with a smarter and more knowledgeable model that provides greater\n",
    "accuracy across various tasks. In particular, GPT-4 introduces Vision as a new feature, that enables\n",
    "it to process image inputs, making it useful in a wider range of applications.\n",
    "\n",
    "\n",
    "#### Function calling\n",
    "\n",
    "The GPT based models are capable of calling previously specified functions in response to user\n",
    "actions or prompts by calling external APIs to retrieve data or to automate procedures like sending\n",
    "an email or extracting and sorting data from a document.\n",
    "\n",
    "#### Assistants\n",
    "\n",
    "The AI Assistants functionality leverages the use of the GPT models alongside function calling\n",
    "and other tools like file retrieval and code interpreter to allow users to create custom assistants\n",
    "that fulfil more specific tasks based on the provided instructions.\n",
    "\n",
    "### 2.2.2  Other models\n",
    "\n",
    "#### DALL-E\n",
    "\n",
    "The DALL-E model is capable of generating images from natural-language text descriptions, as\n",
    "well as modifying existing ones by feeding the model instructions from a text prompt.\n",
    "\n",
    "#### TTS\n",
    "\n",
    "The Text-To-Speech model is capable of converting text into a natural sounding speech. Note\n",
    "that it currently only supports the English language.\n",
    "\n",
    "#### Whisper\n",
    "\n",
    "Whisper does the opposite of the TTS model: it takes an audio input and then transcribes it\n",
    "into text. Unlike text to speech, Whisper is capable of understanding multiple languages, as such\n",
    "it can be used to identify the input language and translate the contents of the speech into English.\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "Text embeddings are vectorial representations of strings of text, such as words or phrases. By\n",
    "comparing two or more vectors we can infer their similarity. This mechanism is highly useful in\n",
    "applications such as search engines or product recommendations due to it’s ability of evaluating\n",
    "similarity between text strings.\n",
    "\n",
    "#### Moderation\n",
    "\n",
    "OpenAI’s Moderation model is designed to verify if a certain piece of text includes any content\n",
    "that could be classified as hateful, violent, sexual, harmful or otherwise inappropriate. Whilst\n",
    "OpenAI’s own use of the model aims to ensure that content complies with their usage policies [3],\n",
    "this model is suitable for any application that aims to ensure a safe digital environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Setup/Installation\n",
    "\n",
    "## 3.1 Creating an OpenAI Account\n",
    "\n",
    "In this section,we will guide you through the process of setting up an OpenAI account. Whether\n",
    "you’re a developer, researcher, or simply curious about AI, having your own account opens the\n",
    "door to the vast possibilities of artificial intelligence.  \n",
    "\n",
    "Firstly, navigate to the OpenAI website [3] to create or log into your account.\n",
    "\n",
    "![OpenAI Website Access](imgs/setup_imgs/passo1.png)\n",
    "\n",
    "Upon logging in with your email, you will be presented with two options: ChatGPT and API. Select the API option to access the documentation.\n",
    "\n",
    "![Select the API Option](imgs/setup_imgs/passo7.png)\n",
    "\n",
    "Congratulations! You have now successfully created an operational OpenAI account.\n",
    "\n",
    "![OpenAI Account Creation Confirmation](imgs/setup_imgs/passo_8.png)\n",
    "\n",
    "## 3.2 Setting up Your Development Environment\n",
    "\n",
    "Setting up a proper development environment is crucial for working efficiently with AI appli-\n",
    "cations. Ensure that you have the necessary tools and libraries installed on your system. For most\n",
    "AI development tasks, Python is the recommended programming language due to its extensive\n",
    "ecosystem of AI and machine learning libraries. So, that is exactly what we are going to help you\n",
    "with in this sub-section.\n",
    "\n",
    "### 3.2.1 Windows\n",
    "\n",
    "The first step is accessing python’s official website and downloading it. In case you are not sure\n",
    "if you have already installed Python in the past, just type \"cmd\" in your search-bar and then type\n",
    "\"python\". If you are having trouble installing, maybe try checking Python’s beginners guide [8].\n",
    "\n",
    "![OpenAI Account Creation Confirmation](imgs/setup_imgs/step1.png)\n",
    "\n",
    "Once installed, you are going to create a virtual environment, as it is good practise to avoid\n",
    "conflicts with other installed libraries.  \n",
    "\n",
    "Insert one of the following command in your command prompt:\n",
    "\n",
    "```\n",
    "python -m venv openai-env\n",
    "\n",
    "python3 -m venv openai-env\n",
    "```\n",
    "\n",
    "Now, after creating the virtual environment, you need to activate it:  \n",
    "\n",
    "`openai-env\\Scripts\\Available at: activate`\n",
    "\n",
    "\n",
    "After this step you should be able to see \"openai-env\" to the left of the cursor input section.  \n",
    "\n",
    "Once you have Python installed and (optionally) set up a virtual environment, the OpenAI\n",
    "Python library can be installed. From the command prompt, run:  \n",
    "\n",
    "`pip install --upgrade openai`\n",
    "\n",
    "Once this completes, running ’pip list’ will show you the Python libraries you have installed in\n",
    "your current environment, which should confirm that the OpenAI Python library was successfully\n",
    "installed.  \n",
    "\n",
    "Now we are going to setup your API key. If you don’t have an API key yet then you’ll have to\n",
    "follow the instruction in API section  \n",
    "\n",
    "Open your command prompt and then insert the following command:  \n",
    "\n",
    "`setx OPENAI_API_KEY \"your-api-key-here`\n",
    "\n",
    "In order to make this key setup permanent, you ought to access Environment Variables, for\n",
    "that you just need to search for it in your windows search bar. Click on \"New\" and then set  \n",
    "\n",
    "**\"OPENAI_API_KEY\"**  \n",
    "\n",
    "as the variable name and your API key as the value.  \n",
    "\n",
    "### 3.2.2 Linux\n",
    "\n",
    "Firstly, open your terminal and introduce the following command in order to download python:\n",
    "In case you are working on Debian or Ubuntu:  \n",
    "\n",
    "`apt install python3 python3-dev`\n",
    "\n",
    "In case you are working on Red Hat, CentOS, or Fedora:\n",
    "\n",
    "`dnf install python3 python3-devel`\n",
    "\n",
    "If you are having trouble installing, maybe try checking Python’s beginners guide [8].  \n",
    "\n",
    "Once installed, you are going to create a virtual environment, as it is good practise to avoid\n",
    "conflicts with other installed libraries.  \n",
    "\n",
    "Insert one of the following command in your terminal:  \n",
    "\n",
    "```\n",
    "python -m venv openai-env\n",
    "\n",
    "python3 -m venv openai-env\n",
    "```\n",
    "\n",
    "If you can’t use none of these commands above because of this error: \"The virtual environment\n",
    "was not created successfully because ensurepip is not available\" then, try using the following\n",
    "command (after this command you have to re-insert one of the commands above):\n",
    "\n",
    "`sudo apt install python3.10-venv`\n",
    "\n",
    "Now, after creating the virtual environment, you need to activate it:\n",
    "\n",
    "`source openai-env/bin/activate`\n",
    "\n",
    "After this step you should be able to see \"openai-env\" to the left of the cursor input section.  \n",
    "\n",
    "Once you have Python installed and (optionally) set up a virtual environment, the OpenAI\n",
    "Python library can be installed. From the terminal, run:  \n",
    "\n",
    "`pip install --upgrade openai`\n",
    "\n",
    "Once this completes, running ’pip list’ will show you the Python libraries you have installed in\n",
    "your current environment, which should confirm that the OpenAI Python library was successfully\n",
    "installed.  \n",
    "\n",
    "Now we are going to setup your API key. If you don’t have an API key yet then you’ll have to\n",
    "follow the instruction in API section.  \n",
    "\n",
    "Go to OpenAI website and access the \"API keys\" section, there you are going to retrieve your\n",
    "API key or create one in case you do not already have.  \n",
    "\n",
    "Then, open your terminal and type the following command:  \n",
    "\n",
    "`export OPENAI_API_KEY=’your-api-key-here’`\n",
    "\n",
    "To save just press Ctrl+O.  \n",
    "If you want to check it did get setup correctly, type  \n",
    "\n",
    "`echo $OPENAI_API_KEY`\n",
    "\n",
    "![OpenAI Account Creation Confirmation](imgs/setup_imgs/step2.png)\n",
    "\n",
    "\n",
    "### 3.2.3 MacOS\n",
    "\n",
    "Firstly install Brew, if not already installed:\n",
    "\n",
    "`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n",
    "\n",
    "Now that you already have Brew, open your terminal and introduce the following command in order to download python:\n",
    "\n",
    "`brew install python`\n",
    "\n",
    "If you are having trouble installing, maybe try checking Python’s beginners guide [8].  \n",
    "\n",
    "Once installed, you are going to create a virtual environment, as it is good practise to avoid conflicts with other installed libraries.  \n",
    "\n",
    "Insert one of the following command in your terminal:  \n",
    "\n",
    "`pip install virtualenv`\n",
    "\n",
    "Create venv:  \n",
    "\n",
    "`virtualenv openai-env`\n",
    "\n",
    "Now, after creating the virtual environment, you need to activate it:\n",
    "\n",
    "`source openai-env/bin/activate`\n",
    "\n",
    "After this step you should be able to see \"openai-env\" to the left of the cursor input section.\n",
    "\n",
    "Once you have Python installed and (optionally) set up a virtual environment, the OpenAI Python library can be installed. From the terminal, run:\n",
    "\n",
    "`pip install --upgrade openai`\n",
    "\n",
    "    \n",
    "Once this completes, running 'pip list' will show you the Python libraries you have installed in your current environment, which should confirm that the OpenAI Python library was successfully installed.\n",
    "\n",
    "Now we are going to setup your API key. If you don't have an API key yet then you'll have to follow the instruction in API section:\n",
    "\n",
    "Go to OpenAI website and access the \"API keys\" section, there you are going to retrieve your API key or create one in case you do not already have.\n",
    "\n",
    "Then, open your terminal and type the following command:\n",
    "\n",
    "`export OPENAI_API_KEY='your-api-key-here'`\n",
    "        \n",
    "To save just press Ctrl+O.\n",
    "    \n",
    "If you want to check it did get setup correctly, type \n",
    "\n",
    "`echo $OPENAI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Tutorial/Functionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all tutorials you are going to need to import OpenAI library for python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this library you need to instantiate a client. To do this you are going to need a key. If no key is indicated in the constructor, OpenAI will default to the environment variable \"OPEN_AI_KEY\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY -> is retrieve from the environment variable\n",
    "client = OpenAI()\n",
    "\n",
    "# API_KEY -> is indicated by the user at the moment of creation\n",
    "client = OpenAI(api_key=\"{YOUR_OPEN_AI_KEY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good practises, we advise that you use a .env file to store your private information, like your key.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Chat Completion\n",
    "\n",
    "Chat Completion, probably one of the simplest OpenAI´s capabilities, takes a list of messages as input and returns an answer.\n",
    "\n",
    "**1 - First import and create an OpenAI client**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Structure our response, passing the gpt model and a few messages to give a brief context to our chatbot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful football assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the Euro back in 2016?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Portugal won the World Cup in 2016.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played, what was the score of the final and who scored in that game?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 4.3 Embeddings -->\n",
    "\n",
    "OpenAI’s text embeddings transforms strings to numbers, that allows to measure the relatedness of text strings. Embeddings are commonly used for:\n",
    "\n",
    "* Search (where results are ranked by relevance to a query string)\n",
    "* Clustering (where text strings are grouped by similarity)\n",
    "* Recommendations (where items with related text strings are recommended)\n",
    "* Anomaly detection (where outliers with little relatedness are identified)\n",
    "* Diversity measurement (where similarity distributions are analyzed)\n",
    "* Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "\n",
    "\n",
    "Like mentioned before, Embeddings have a lot of uses but this tutorial will only focus on how to make the request and show simple method to compare two strings.\n",
    "\n",
    "**Models**\n",
    "\n",
    "Right now there are three available models for Embeddings. The ones with \"-3\" on the name are third generation models.\n",
    "\n",
    "* text-embedding-3-small\t\n",
    "* text-embedding-3-large\n",
    "* text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Import the necessary libraries and start the client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Retrieve the embeddings**\n",
    "\n",
    "*model* - changes the model use to retieve information  \n",
    "*input* - string message you want to retrieve the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = client.embeddings.create(\n",
    "    input=\"We are testing to see if this string has any similarities to another one.\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embeddings1 = response1.data[0].embedding\n",
    "\n",
    "print(f\"Embeddings Example - {embeddings1[0]}, {embeddings1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare two strings using embeddings and cosine similarity\n",
    "\n",
    "**1 - Import numpy and cosine_similarity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Retrieve the embeddings for the second string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = client.embeddings.create(\n",
    "    input=\"We're experimenting to determine if this string bears resemblance to another.\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embeddings2 = response2.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Convert the embeddings to numpy arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = np.array(embeddings1).reshape(1, -1)\n",
    "embeddings2 = np.array(embeddings2).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 - Calculate the similarity**\n",
    "\n",
    "The closer the score is to 1 the similar it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "final_score = float(format(similarity_score[0][0], \".2f\"))\n",
    "\n",
    "print(f\"Similarity: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4  Vision\n",
    "\n",
    "Vision API from OpenAI, based on GPT-4 Turbo, is an enormous jump in AI capability. This functionality brought AI the capability to take pictures as input, comprehend them and answer questions about them. This development guarantees to revolutionize the way we interact and use AI. \n",
    "\n",
    "Vision can accept images through links or even by passing its base64 encoded image. Unfortunately, this feature is only available to ChatGPT-4, so we will be addressing this tutorial at the end of our article as an extra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, in order to start using this functionality, you need to import the necessary packages. In this case, we only need to import the `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it´s time to create a `client` in order to request the `response` from `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the `response` is structered, passing the role, content and the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Image Generation (DALL-E)\n",
    "\n",
    "\n",
    "\n",
    "The open AI image API provides three methods for interacting with images, namely creating images from scratch using a text prompt (DALL-E 3 and DALL-E 2), creating edited versions of images by having the model change some areas of a pre-existing image based on a new text prompt (DALL-E 2 only) and creating variations of an existing image (DALL-E 2 only).\n",
    "\n",
    "This guide covers the basics of the API methods with useful code examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Image Generation \n",
    "\n",
    "The image generation method allows you to create an original image with a text prompt.\n",
    "\n",
    "Initially, it is necessary to import the display and Image functions from the IPython.display module, in order to display the image inside a Jupyter notebook.\n",
    "\n",
    "It also imports os, used for operating system operations, as explained earlier for creating a venv environment, and imports the OpenAI class from the openai module, an interface for using the API.\n",
    "\n",
    "The code creates an instance of the OpenAI() client.\n",
    "A request is made to the API to generate an image, with the parameters:\n",
    "\n",
    "* __model :__ specifies the model used to generate the image, in this case \"dall-e-2\", which specialises in generating images based on textual descriptions;\n",
    "* __prompt :__ descriptive text that will serve as input for the model to generate the image, in this example, \"A cat inside a car\";\n",
    "* __size :__ specifies the size, 1024x1024 pixels;\n",
    "* __quality :__  sets the quality, \"standard\". When using DALL-E 3 it is possible to set quality: \"hd\", i.e. fine detail. However, standard quality square images are generated more quickly;\n",
    "* __n :__ defines the number of images generated.\n",
    "\n",
    "After the API generates the image, the image URL is extracted from the response and stored in the image_url variable.\n",
    "Finally, the image URL is printed and the image is displayed using the display function with the Image class, passing the URL as a parameter and setting the image width to 500 pixels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "  model=\"dall-e-2\",\n",
    "  prompt=\"A cat inside a car\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "print(image_url)\n",
    "display(Image(url=image_url, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Edits (DALLE 2 only)\n",
    "\n",
    "Also known as \"inpainting\", the image editing endpoint allows you to edit an image by loading an image and a mask indicating the areas that should be replaced, you can use tools such as [GIMP](https://www.gimp.org/) or [Photoshop](https://www.adobe.com/pt/products/photoshop/landpa.html?gclid=CjwKCAjw57exBhAsEiwAaIxaZrgKDCM6FAEGOCavdUMdwMkJnL6o9cWGXjdYdjYN1DoN5HeWLj6-nBoCTjgQAvD_BwE&mv=search&s_kwcid=AL!3085!3!340859421278!e!!g!!adobe%20photoshop!1447265685!53212492301&mv=search&mv2=paidsearch&sdid=2XBSBWBF&ef_id=CjwKCAjw57exBhAsEiwAaIxaZrgKDCM6FAEGOCavdUMdwMkJnL6o9cWGXjdYdjYN1DoN5HeWLj6-nBoCTjgQAvD_BwE:G:s&s_kwcid=AL!3085!3!340859421278!e!!g!!adobe%20photoshop!1447265685!53212492301&gad_source=1), to erase a certain area of the image. \n",
    "The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the complete new image, not just the deleted area.\n",
    "\n",
    "The image and mask sent must be square PNG images of less than 4 MB and must also have the same dimensions. The non-transparent areas of the mask are not used to generate the output, so they don't necessarily have to match the original image as in the following example, which shows the original image, the mask and the resulting image after the editing process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"pt\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Imagens</title>\n",
    "<style>\n",
    "    figure {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Espaçamento entre as imagens */\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/original.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Original</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/mask.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Mask</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/output.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Output</figcaption>\n",
    "</figure>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "from IPython.display import display, Image\n",
    "\n",
    "response = client.images.edit(\n",
    "  model=\"dall-e-2\",\n",
    "  image=open(\"Tutorials/DALLE/original.png\", \"rb\"),\n",
    "  mask=open(\"Tutorials/DALLE/mask.png\", \"rb\"),\n",
    "  prompt=\"A hand holding a sandwich\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "\n",
    "print(image_url)\n",
    "display(Image(url=image_url, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Variations (DALL·E 2 only)\n",
    "\n",
    "The image variations endpoint allows you to generate a variation of a given image.\n",
    "\n",
    "Similar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.create_variation(\n",
    "  model=\"dall-e-2\",\n",
    "  image=open(\"Tutorials/DALLE/original.png\", \"rb\"),\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "\n",
    "print(image_url)\n",
    "display(Image(url=image_url, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images below correspond to a possible example of the endpoint variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"pt\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Imagens</title>\n",
    "<style>\n",
    "    figure {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Espaçamento entre as imagens */\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/original.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Original</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Tutorials/DALLE/variation.png\" width=\"350\" height=\"350\" alt=\"\">\n",
    "    <figcaption>Variation</figcaption>\n",
    "</figure>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 TTS Tutorial\n",
    "\n",
    "In this tutorial you will learn how to integrate text-to-speech feature from open-ai in your projects.\n",
    "\n",
    "| Available voices | Output Formats           |\n",
    "|------------------|--------------------------|\n",
    "| Alloy            | **mp3** - default format|\n",
    "| Echo             | **opus**                 |\n",
    "| Fable            | **aac**                  |\n",
    "| Onyx             | **flac**                 |\n",
    "| Nova             | **waw**                  |\n",
    "| Shimmer          | **pcm**                  |\n",
    "\n",
    "\n",
    "Voice only changes the tone and the \"person\" who is speaking. TTS only produces english audio files.\n",
    "\n",
    "\n",
    "There are two models available tts right now: **tts-1** and **tts-1-hd**. If you want lower latency **tts-1** is recommended, but it comes with lower quality than **tts-1-hd**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Importing the necessary libraries and creating the client**\n",
    "\n",
    "pathlib - offers classes that represent a filesystem path  \n",
    "IPython - serves to display the audio in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import IPython\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Create a path to save the audio file**\n",
    "\n",
    "In here you can choose the format you want your audio file to be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file_path = Path(f\"Tutorials/TTS/tts_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Generate the audio**\n",
    "\n",
    "The endpoint will receive a model, a voice and your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"shimmer\",\n",
    "  input=\"Hey, I'm a student in Lincenciatura de Engenharia de Telecomunicações e Informática in Instituto Superior de Engenharia do Porto, and I'm doing a tutorial on how to use open-a.i in my projects!\"\n",
    ")\n",
    "\n",
    "response.write_to_file(speech_file_path)\n",
    "\n",
    "IPython.display.Audio(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Whisper\n",
    "\n",
    "In this tutorial you will learn how to use Whisper to transcribe text from audio files as well translate it into English.\n",
    "\n",
    "If you **haven't set up your OpenAI API key** as a global system variable you can set it up now by pasting it into the **.env** file and running the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, just runs this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the audio file**\n",
    "\n",
    "Feel free to try different audio files as well as add/record your own. Files must be of one of these types: mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n",
    "\n",
    "**Note that files greater than 25MB will need to be segmented using [additional libraries](https://platform.openai.com/docs/guides/speech-to-text/longer-inputs).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"Tutorials/Whisper/audio.wav\", \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Transcribe an audio file**\n",
    "\n",
    "The transcription endpoint will take the input audio and transcribe it into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    ")\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how **response_format=\\\"text\\\"**? To get additional information to get additional information try changing it to **verbose_json**.\n",
    "\n",
    "You should now receive a json response with additional parameters. One of which, the **language** parameter, includes the detected language from the input file.\n",
    "\n",
    "**Note:** If the language is not being properly detected, which may negatively impact transcription, you can add an additional parameter stating it according to the  [ISO-639-1 format](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n",
    "\n",
    "```\n",
    "(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    "    language=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "We can modify our code to reflect this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    response_format=\"verbose_json\"\n",
    ")\n",
    "print(f\"Detected language: {transcription.language}\")\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Translation**\n",
    "\n",
    "Using the translation endpoint we can translate the contents of the audio file to English (currently this the only available language for translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = client.audio.translations.create(\n",
    "    model = \"whisper-1\", \n",
    "    file = audio_file,\n",
    "    response_format=\"text\"\n",
    ")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Moderation\n",
    "\n",
    "The moderation endpoint is a tool you can use to check whether text is potentially harmful. It can be used to identify content that could be harmful and take action.\n",
    "\n",
    "The templates classify the following categories:\n",
    "* *__Hate__* - Content that expresses or promotes hatred based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status or caste. Hateful content directed at unprotected groups constitutes harassment.\n",
    "* *__Hate/Threatening__* - Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.\n",
    "* *__Harassment__* - Content that expresses, incites, or promotes harassing language towards any target.\n",
    "* *__Harassment/Threatening__* - Harassment content that also includes violence or serious harm towards any target.\n",
    "* *__Self-harm__* - Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.\n",
    "* *__Self-harm/Intent__* - Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.\n",
    "* *__Self-harm/Instructions__* - Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.\n",
    "* *__Sexual__* - Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).\n",
    "* *__Sexual/Minors__* - Sexual content that includes an individual who is under 18 years old.\n",
    "* *__Violence__* - Content that depicts death, violence, or physical injury.\n",
    "* *__Violence/Graphic__* - Content that depicts death, violence, or physical injury in graphic detail.\n",
    "\n",
    "\n",
    "To obtain a classification for a piece of text, a request is made to the moderation endpoint, as shown in the following code fragment.\n",
    "Firstly, the OpenAI class from the openai module is imported, then an instance of the class is created and assigned to a variable, configuring the client to interact with the OpenAI API.\n",
    "\n",
    "With the 'moderations.create()' method, the text is sent for moderation.\n",
    "\n",
    "The response from the API will be a JSON object with information about the moderation of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.moderations.create(input=\"I hate Chinese and black people \")\n",
    "\n",
    "output = response.results[0]\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the endpoint response structure. It returns the following fields:\n",
    "\n",
    "* *__flagged__*: Set to true if the model classifies the content as potentially harmful, false otherwise.\n",
    "\n",
    "* *__categories__*: Contains a dictionary of violation flags for each category. The value will be true if the model flags the corresponding category as violated, false otherwise.\n",
    "* *__category_scores__*: Contains a dictionary of raw scores per category issued by the model, denoting the model's confidence that the input violates the OpenAI policy for the category. The value is between 0 and 1, with higher values denoting greater confidence. The scores should not be interpreted as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"id\": \"modr-XXXXX\",\n",
    "    \"model\": \"text-moderation-007\",\n",
    "    \"results\": [\n",
    "        {\n",
    "            \"flagged\": true,\n",
    "            \"categories\": {\n",
    "                \"sexual\": false,\n",
    "                \"hate\": false,\n",
    "                \"harassment\": false,\n",
    "                \"self-harm\": false,\n",
    "                \"sexual/minors\": false,\n",
    "                \"hate/threatening\": false,\n",
    "                \"violence/graphic\": false,\n",
    "                \"self-harm/intent\": false,\n",
    "                \"self-harm/instructions\": false,\n",
    "                \"harassment/threatening\": true,\n",
    "                \"violence\": true\n",
    "            },\n",
    "            \"category_scores\": {\n",
    "                \"sexual\": 1.2282071e-6,\n",
    "                \"hate\": 0.010696256,\n",
    "                \"harassment\": 0.29842457,\n",
    "                \"self-harm\": 1.5236925e-8,\n",
    "                \"sexual/minors\": 5.7246268e-8,\n",
    "                \"hate/threatening\": 0.0060676364,\n",
    "                \"violence/graphic\": 4.435014e-6,\n",
    "                \"self-harm/intent\": 8.098441e-10,\n",
    "                \"self-harm/instructions\": 2.8498655e-11,\n",
    "                \"harassment/threatening\": 0.63055265,\n",
    "                \"violence\": 0.99011886\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Exercise A\n",
    "\n",
    "Embeddings have a lot of uses, when combined with other APIs can do even more. One example is using embeddings with chat completion to extract information from a pdf and then create a function to ask anything about the document.\n",
    "\n",
    "In the following exercise you will create a program that retrieves information from a pdf and answer questions about it. In order to achieve this you must:\n",
    "*   Convert a pdf file to embeddings and save them in a csv file \n",
    "*   Use embeddings to search a user query in the csv file\n",
    "*   Send that information to chat completions\n",
    "  \n",
    "The pdf used in this exercise will be `LETI_SISTCA_2023_24_Team2_OpenAI.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start by importing the requiring dependecies and initialize the client and creating some constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd  \n",
    "import re \n",
    "import tiktoken \n",
    "import PyPDF2\n",
    "\n",
    "import ast\n",
    "from scipy import spatial  \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"Contents\",\n",
    "    \"List of Tables\",\n",
    "    \"List of Figures\",\n",
    "    \"References\",\n",
    "]\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "BATCH_SIZE = 1000   \n",
    "\n",
    "# TODO #1: Create consts for GPT_MODEL and EMBEDDING_MODEL (small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Logic to Extract the information from the pdf**\n",
    "\n",
    "This is a simple logic to extract the necessary information from the pdf we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def split_sections_from_pdf(pdf_text):\n",
    "    title = []\n",
    "    text = []\n",
    "    ignore = True\n",
    "    current_section = \"I.N.I.T.I.A.L-V.A.L.U.E\"\n",
    "    for line in pdf_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        if is_new_section(line):\n",
    "            ignore = ignore_section(line)\n",
    "            if ignore:\n",
    "                continue\n",
    "            title.append(line)\n",
    "            if current_section != \"I.N.I.T.I.A.L-V.A.L.U.E\":\n",
    "                text.append(current_section)\n",
    "            current_section = \"\"\n",
    "        else:\n",
    "            if not ignore:\n",
    "                current_section += \" \" + line\n",
    "    if current_section:\n",
    "        text.append(current_section)\n",
    "        \n",
    "    \n",
    "    sections = [(title),(text)]\n",
    "    return sections\n",
    "\n",
    "def ignore_section(line):\n",
    "    if any(section in line for section in SECTIONS_TO_IGNORE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_new_section(line):\n",
    "    pattern = r\"\\d+\\.\\d+(?:\\.\\d+)? [A-Z].*?\"\n",
    "    \n",
    "    if line.strip().count('.') > 7:\n",
    "        return False\n",
    "    if re.match(pattern, line.strip()):\n",
    "        return True \n",
    "    if any(section in line for section in SECTIONS_TO_IGNORE):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def clean_section(section):\n",
    "\n",
    "    titles = section[0]\n",
    "    text = section[1]\n",
    "    \n",
    "    for line in text:\n",
    "        line = re.sub(r\"\\[\\d+\\]\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\d\\d+\\]\", \"\", line)\n",
    "        line = line.strip()\n",
    "\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "def num_tokens(text, model = GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def halved_by_delimiter(string, delimiter = \"\\n\"):\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  \n",
    "    elif len(chunks) == 2:\n",
    "        return chunks \n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "    \n",
    "    \n",
    "def truncated_string(string, model, max_tokens, print_warning = True,):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(title, text, max_tokens = 1000, model = GPT_MODEL, max_recursion = 5):\n",
    "\n",
    "    string = \"\\n\\n\".join(title + text)\n",
    "    num_tokens_in_string = num_tokens(string, model)\n",
    "\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model, max_tokens)]\n",
    "\n",
    "    else:\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_strings = split_strings_from_subsection(title, half,max_tokens,model,max_recursion - 1)\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "            \n",
    "    return [truncated_string(string, model, max_tokens)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call all functions to retrieve the clean pdf sections to then split it into strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO #2: Call the previous created functions\n",
    "pdf_text = ...\n",
    "pdf_sections = ...\n",
    "cleaned_sections = ...\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "strings = []\n",
    "titles = cleaned_sections[0]\n",
    "texts = cleaned_sections[1]\n",
    "for i in range(len(titles)):\n",
    "    strings.extend(split_strings_from_subsection(titles[i], texts[i], max_tokens=MAX_TOKENS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming the information to embeddings and saving it to a CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for batch_start in range(0, len(strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = strings[batch_start:batch_end]\n",
    "    \n",
    "    # TODO #3: Make a request to the embeddings API with the batch as input\n",
    "    response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    \n",
    "    for i, be in enumerate(response.data):\n",
    "        assert i == be.index  \n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame({\"text\": strings, \"embedding\": embeddings})\n",
    "\n",
    "\n",
    "SAVE_PATH = \"SISTCA_TEAM2.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is done, now we need to create a function so GPT can awnser anything about the pdf using the saved embeddings**\n",
    "\n",
    "**Change the Embedding Model and read the CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO #4: Change the EMBEDDING_MODEL (ada)\n",
    "\n",
    "\n",
    "# TODO #5: Create a variable with the CSV file path\n",
    "\n",
    "\n",
    "df = pd.read_csv(embeddings_path)\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to compare the relatedness off the strings with the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(query, df , relatedness_fn = lambda x, y: 1 - spatial.distance.cosine(x, y), top_n = 100) :\n",
    "    \n",
    "    # TODO 6: Make a request to the embeddings API with the query as input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n",
    "\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"open ai\", df, top_n=5)\n",
    "\n",
    "\n",
    "def num_tokens(text, model = GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def query_message(query,df, model, token_budget):\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below articles on the document about OpenAI made by Team 2, composed by Patrícia Sousa, Carlos Alves, Jose Leal and Tiago Ribeiro, for SISTCA to answer the subsequent question. If the answer cannot be found in the articles, write \"Sorry, the information you seek cannot be found in the document in question.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nPDF article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the final function**  \n",
    "\n",
    "\n",
    "If the chat completions does not have information to answer the query, like defined, it will say _\"Sorry, the information you seek cannot be found in the document in question.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, df = df, model = GPT_MODEL, token_budget = 4096 - 500, print_message = False):\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about the document made by Team2 for SISTCA about OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    \n",
    "    # TODO 7: Make a request to the Chat Completions API\n",
    "    \n",
    "    \n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the ask function to verify it**\n",
    "\n",
    "Because everytime you run ask you give a new prompt to chat completions the awnsers may very for one attempt to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask(\"Scientific/technological background\")) \n",
    "print(ask(\"Give me the authors\")) \n",
    "print(ask(\"What can you tell me about the document\")) \n",
    "print(ask(\"Give me the document structure\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Exercise B\n",
    "\n",
    "Despite, in it's current state, only allowing English translation, we can increment the Whisper API with other APIs so as to be able to translate audio into other languages.\n",
    "By using the standard GPT model in the Chat Completions API we can translate text to and from any language.\n",
    "\n",
    "In the following exercise you will create a program that translates an audio message into any other language. In order to achieve this you must:\n",
    "\n",
    " - Use Whisper to transcribe the original audio;\n",
    " - Translate the resulting transcription into any language using Chat Completions;\n",
    " - Return the translated audio using Text-To-Speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start by importing the required dependencies and initializing the client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Whisper to transcribe the original audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #1: load the audio file\n",
    "\n",
    "\n",
    "\n",
    "# TODO #2: transcribe contents and detect the input language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate the transcribed text into any language using Chat Completions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #3: translate the transcription to another language using chatCompletions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the translated audio using Text-To-Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO #4: output the translated audio file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the output audio file\n",
    "output_file_path = Path(__file__).parent / f\"translation.mp3\"\n",
    "translated_audio.write_to_file(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the output\n",
    "import IPython.display as display\n",
    "\n",
    "print(f\"Detected language: {detected_language} \\n\")\n",
    "print(f\"Transcribed text: {transcribed_text} \\n\")\n",
    "\n",
    "print(f\"Translated text: {translated_text} \\n\")\n",
    "print(f\"Translated audio file saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "display.Audio(speech_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] IBM, “What is an ai model?,” IBM. Accessed: 2024-03-28.  \n",
    "[2] R. Merritt, “What is a transformer model?,” NVIDIA Blog, mar 2022. (Accessed:\n",
    "2024-03-28.  \n",
    "[3] “Openai.” Available at: https://openai.com/. Accessed: 2024-03-27.  \n",
    "[4] xAI, “xai grok.” Available at: https://grok.x.ai/. (Accessed: 2024-03-27).  \n",
    "[5] Anthropic, “Anthropic home.” Available at: https://www.anthropic.com/. (Ac-\n",
    "cessed: 2024-03-27).  \n",
    "[6] Google, “Deepmind technologies.” Available at:\n",
    "https://deepmind.google/technologies/. (Accessed: 2024-03-27).  \n",
    "[7] Cohere, “Cohere home.” Available at: https://cohere.com/. (Accessed: 2024-03-\n",
    "27).  \n",
    "[8] “Python.” Available at: https://wiki.python.org/. (Accessed: 2024-03-29).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
