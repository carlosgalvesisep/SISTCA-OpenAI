{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A\n",
    "\n",
    "Embeddings have a lot of uses, when combined with other APIs can do even more. One example is using embeddings with chat completion to extract information from a pdf and then create a function to ask anything about the document.\n",
    "\n",
    "In the following exercise you will create a program that retrieves information from a pdf and answer questions about it. In order to achieve this you must:\n",
    "*   Convert a pdf file to embeddings and save them in a csv file \n",
    "*   Use embeddings to search a user query in the csv file\n",
    "*   Send that information to chat completions\n",
    "\n",
    "The pdf used in this exercise will be `LETI_SISTCA_2023_24_Team2_OpenAI.pdf`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start by importing the requiring dependecies and initialize the client and creating constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd  \n",
    "import re \n",
    "import tiktoken \n",
    "import PyPDF2\n",
    "\n",
    "import ast\n",
    "from scipy import spatial  \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"Contents\",\n",
    "    \"List of Tables\",\n",
    "    \"List of Figures\",\n",
    "    \"References\",\n",
    "]\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "BATCH_SIZE = 1000   \n",
    "\n",
    "# TODO #1: Create consts for GPT_MODEL and EMBEDDING_MODEL (small)\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Logic to Extract the information from the pdf**\n",
    "\n",
    "This is a simple logic to extract the necessary information from the pdf we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def split_sections_from_pdf(pdf_text):\n",
    "    title = []\n",
    "    text = []\n",
    "    ignore = True\n",
    "    current_section = \"I.N.I.T.I.A.L-V.A.L.U.E\"\n",
    "    for line in pdf_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        if is_new_section(line):\n",
    "            ignore = ignore_section(line)\n",
    "            if ignore:\n",
    "                continue\n",
    "            title.append(line)\n",
    "            if current_section != \"I.N.I.T.I.A.L-V.A.L.U.E\":\n",
    "                text.append(current_section)\n",
    "            current_section = \"\"\n",
    "        else:\n",
    "            if not ignore:\n",
    "                current_section += \" \" + line\n",
    "    if current_section:\n",
    "        text.append(current_section)\n",
    "        \n",
    "    \n",
    "    sections = [(title),(text)]\n",
    "    return sections\n",
    "\n",
    "def ignore_section(line):\n",
    "    if any(section in line for section in SECTIONS_TO_IGNORE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_new_section(line):\n",
    "    pattern = r\"\\d+\\.\\d+(?:\\.\\d+)? [A-Z].*?\"\n",
    "    \n",
    "    if line.strip().count('.') > 7:\n",
    "        return False\n",
    "    if re.match(pattern, line.strip()):\n",
    "        return True \n",
    "    if any(section in line for section in SECTIONS_TO_IGNORE):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def clean_section(section):\n",
    "\n",
    "    titles = section[0]\n",
    "    text = section[1]\n",
    "    \n",
    "    for line in text:\n",
    "        line = re.sub(r\"\\[\\d+\\]\", \"\", line)\n",
    "        line = re.sub(r\"\\[\\d\\d+\\]\", \"\", line)\n",
    "        line = line.strip()\n",
    "\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "def num_tokens(text, model = GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def halved_by_delimiter(string, delimiter = \"\\n\"):\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  \n",
    "    elif len(chunks) == 2:\n",
    "        return chunks \n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "    \n",
    "    \n",
    "def truncated_string(string, model, max_tokens, print_warning = True,):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(title, text, max_tokens = 1000, model = GPT_MODEL, max_recursion = 5):\n",
    "\n",
    "    string = \"\\n\\n\".join(title + text)\n",
    "    num_tokens_in_string = num_tokens(string, model)\n",
    "\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model, max_tokens)]\n",
    "\n",
    "    else:\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_strings = split_strings_from_subsection(title, half,max_tokens,model,max_recursion - 1)\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "            \n",
    "    return [truncated_string(string, model, max_tokens)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call all functions to retrieve the clean pdf sections to then split it into strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Truncated string from 2534 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 2090 tokens to 1600 tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO #2: Call the previous created functions\n",
    "pdf_text = extract_text_from_pdf(\"LETI_SISTCA_2023_24_Team2_OpenAI.pdf\")\n",
    "pdf_sections = split_sections_from_pdf(pdf_text)\n",
    "cleaned_sections = clean_section(pdf_sections)\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "strings = []\n",
    "titles = cleaned_sections[0]\n",
    "texts = cleaned_sections[1]\n",
    "for i in range(len(titles)):\n",
    "    strings.extend(split_strings_from_subsection(titles[i], texts[i], max_tokens=MAX_TOKENS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming the information to embeddings and saving it to a CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for batch_start in range(0, len(strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = strings[batch_start:batch_end]\n",
    "    \n",
    "    # TODO #3: Make a request to the embeddings API with the batch as input\n",
    "    response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    \n",
    "    for i, be in enumerate(response.data):\n",
    "        assert i == be.index  \n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame({\"text\": strings, \"embedding\": embeddings})\n",
    "\n",
    "\n",
    "SAVE_PATH = \"SISTCA_TEAM2.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is done, now we need to create a function so GPT can awnser anything about the pdf using the saved embeddings**\n",
    "\n",
    "**Change the Embedding Model and read the CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO #4: Change the EMBEDDING_MODEL (ada)\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# TODO #5: Create a variable with the CSV file path\n",
    "embeddings_path = \"SISTCA_TEAM2.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to compare the relatedness off the strings with the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(query, df , relatedness_fn = lambda x, y: 1 - spatial.distance.cosine(x, y), top_n = 100) :\n",
    "    \n",
    "    # TODO 6: Make a request to the embeddings API with the query as input\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    \n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n",
    "\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"open ai\", df, top_n=5)\n",
    "\n",
    "\n",
    "def num_tokens(text, model = GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def query_message(query,df, model, token_budget):\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below articles on the document about OpenAI made by Team 2, composed by Patrícia Sousa, Carlos Alves, Jose Leal and Tiago Ribeiro, for SISTCA to answer the subsequent question. If the answer cannot be found in the articles, write \"Sorry, the information you seek cannot be found in the document in question.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nPDF article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the final function**  \n",
    "\n",
    "\n",
    "If the chat completions does not have information to answer the query, like defined, it will say _\"Sorry, the information you seek cannot be found in the document in question.\"_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, df = df, model = GPT_MODEL, token_budget = 4096 - 500, print_message = False):\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about the document made by Team2 for SISTCA about OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    \n",
    "    # TODO 7: Make a request to the Chat Completions API\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the ask function to verify it**\n",
    "\n",
    "Because everytime you run ask you give a new prompt to chat completions the awnsers may very for one attempt to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, the information you seek cannot be found in the document in question.\n",
      "The authors of the document about OpenAI made by Team 2 for SISTCA are Patrícia Sousa, Carlos Alves, Jose Leal, and Tiago Ribeiro.\n",
      "The document made by Team 2 for SISTCA about OpenAI covers various topics related to OpenAI, including Chat Completions, AI Companies, Assistants, Vision, Whisper, TTS (Text-to-Speech), and exercises related to challenges and integration of text-to-speech features. It also provides information on setting up OpenAI API keys and integrating text-to-speech features from OpenAI into projects.\n",
      "The document structure includes sections such as Chat Completions, State-of-the-art AI companies, Exercise B 196 Challenge, Assistants, Vision, Exercise A, MacOS, Document Structure, and Whisper.\n"
     ]
    }
   ],
   "source": [
    "#Test the function: Be aware that the awnsers are not the same everytime you do the request\n",
    "print(ask(\"Scientific/technological background\")) \n",
    "print(ask(\"Give me the authors\")) \n",
    "print(ask(\"What can you tell me about the document\")) \n",
    "print(ask(\"Give me the document structure\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
