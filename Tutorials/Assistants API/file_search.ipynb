{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Search\n",
    "\n",
    "File search is a tool that complements the Assistants API. It allows users to send their documents and question the assistant about them. The documents sent by the user are transformed in embeddings and stored in vector stores. File search uses both keyword and vector search to retrieve relevant information.\n",
    "\n",
    "\n",
    "This feature implements good retrieval pratices to extract the necessary data to answer the user query:\n",
    "* Rewrites user queries to optimize them for search.\n",
    "* Breaks down complex user queries into multiple searches that can run in parallel.\n",
    "* Runs both keyword and semantic searches across both assistant and thread vector stores.\n",
    "* Reranks search results to pick the most relevant ones before generating the final response.\n",
    "\n",
    "\n",
    "File search supports a variety of different file formats. For this tutorial, a simple txt containing some wikipedia articles was the source of the information.\n",
    "\n",
    "* .c\t\n",
    "* .cs\t\n",
    "* .cpp\t\n",
    "* .doc\t\n",
    "* .docx\t\n",
    "* .html\t\n",
    "* .java\t\n",
    "* .json\t\n",
    "* .md\t\n",
    "* .pdf\t\n",
    "* .php\t\n",
    "* .pptx\t\n",
    "* .py\t\n",
    "* .py\t\n",
    "* .rb\t\n",
    "* .tex\t\n",
    "* .txt\t\n",
    "* .css\t\n",
    "* .js\t\n",
    "* .sh\t\n",
    "* .ts\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have found information about two artists and their albums:\n",
      "\n",
      "1. **Illenium**\n",
      "   - Albums:\n",
      "     - Ashes (2016)\n",
      "     - Awake (2017)\n",
      "     - Ascend (2019)\n",
      "     - Fallen Embers (2021)\n",
      "     - Illenium (2023)【4:0†source】\n",
      "\n",
      "2. **Yellow Claw**\n",
      "   - Albums:\n",
      "     - Blood for Mercy (2015)\n",
      "     - Los Amsterdam (2017)\n",
      "     - New Blood (2018)\n",
      "     - Never Dies (2020)【4:2†source】\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "# choose the model\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    " \n",
    "# initialize the client\n",
    "client = OpenAI()\n",
    " \n",
    "# create an assistant with the instructions about his job\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Music Information Assistant\",\n",
    "  instructions=\"You are an expert music analyst. Use you knowledge base to answer questions about music artist and their work.\",\n",
    "  model=GPT_MODEL,\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "# create a vector store to store your information\n",
    "vector_store = client.beta.vector_stores.create(name=\"Music Information\")\n",
    "\n",
    "# add files, it is possible to add multiple\n",
    "file_paths = [\"file_search_information.txt\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# upload the files and add them to the vector store\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "# update the assistant with the vector store created\n",
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n",
    "# upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"file_search_information.txt\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "\n",
    "# what do you want to ask/tell about the files\n",
    "message_input = \"How many artists do you know about, and what are their albums?\"\n",
    " \n",
    "# create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": message_input,\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "# run the thread until it's in a terminal state.\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# retrieve the necessary information \n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "# get the answer and print it\n",
    "message_content = messages[0].content[0].text\n",
    "\n",
    "print(message_content.value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
