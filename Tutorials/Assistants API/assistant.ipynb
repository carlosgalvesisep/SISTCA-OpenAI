{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assistants API makes it possible to create AI assistants in applications. An Assistant has instructions and can use models, tools and files to answer the user's questions. The Assistants API currently supports three types of tools: Code Interpreter, File Search and Function Calling.\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.1 Assistant\n",
    "\n",
    "In this example, an Assistant is created with the purpose of being a personal maths tutor.\n",
    "\n",
    "Initially, an Assistant is created, which represents an entity that can be configured to respond to a user's messages using various parameters such as a , `name`, `instructions` and `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from runners.standard_run import std_run\n",
    "from runners.streaming_run import streaming_run\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a Thread is created that represents a conversation between a user and one or more Assistants. A Thread can be created when a user (or their AI application) starts a conversation with their Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message is added to the Thread, the content of the messages that users or applications create is added as objects to the Message Thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `8x + 12 = 14`. Can you help me?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user has added all of the necessary information/messages to the thread you must run the thread to provide them with results.\n",
    "\n",
    "You can run a thread using its id as well as the assistant's id, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    #model = \"gpt-3.5-turbo\" # By default the model defined in the assistant will be used. You can, however, overwrite it here.\n",
    "    instructions = \"Please address the user as SISTCA student.\" # This parameter is optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During its lifecycle, a run can have multiple states:\n",
    "\n",
    "![image.png](https://cdn.openai.com/API/docs/images/diagram-run-statuses-v2.png)\n",
    "[https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps]\n",
    "\n",
    "Therefore, we must check for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queued\n"
     ]
    }
   ],
   "source": [
    "if run.status == 'completed': \n",
    "    messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "    )\n",
    "    print(messages)\n",
    "else:\n",
    "    print(run.status)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SISTCA student, let's solve the equation `8x + 12 = 14` step by step.\n",
      "\n",
      "1. Subtract 12 from both sides:\n",
      "`8x + 12 - 12 = 14 - 12`\n",
      "`8x = 2`\n",
      "\n",
      "2. Divide by 8 to solve for x:\n",
      "`8x / 8 = 2 / 8`\n",
      "`x = 1/4`\n",
      "\n",
      "Therefore, the solution to the equation `8x + 12 = 14` is `x = 1/4`.\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id = thread.id)\n",
    "print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to yield any results, an assistant's thread must be run. This is to say that whenever you're working with the assistant functionality you must have a runner function. \n",
    "\n",
    "Due to the fact that we will be exploring other types of assistants, and as a way of not repeating this code multiple times, we will export as the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_run (thread_id, assistant_id, client):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread_id,\n",
    "        assistant_id = assistant_id,\n",
    "        instructions = \"Please address the user as SISTCA student.\"\n",
    "    )   \n",
    "\n",
    "    if run.status == 'completed': \n",
    "        messages = client.beta.threads.messages.list(thread_id = thread_id)\n",
    "        print(messages.data[0].content[0].text.value)\n",
    "    else:\n",
    "        print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have used ChatGPT before you may have noticed that the output is not returned all at once, like it is here, but rather gradually, in real time.\n",
    "\n",
    "We can achieve this same effect by using a different type of runner, the streaming run.\n",
    "\n",
    "\n",
    "In this run, the streaming_run function is defined, which is responsible for starting the streaming with the assistant. \n",
    "Within this, the 'EventHandler' class is defined, which is inherited from AssistantEventHandler. \n",
    "This class contains the methods responsible for dealing with the different events that occur during streaming: \n",
    "\n",
    "* `on_text_created`: Called when a new text is created by the assistant. This prints 'assistant' to indicate that the assistant is responding.\n",
    "\n",
    "* `on_text_delta`: Called for each text update. This prints the part of the text that was generated (delta.value).\n",
    "\n",
    "* `on_tool_call_created`: Called when the wizard calls a tool. This prints the type of tool call (tool_call.type).\n",
    "\n",
    "* `on_tool_call_delta`:  Called for tool call updates, specifically for the code_interpreter. This prints the code_interpreter input and any outputs, especially logs.\n",
    "\n",
    "\n",
    "Finally, the API client's create_and_stream method is used to start streaming, passing the EventHandler as the event handler.\n",
    "\n",
    "\n",
    "* `thread_id`: and`assistant_id` are the required identifiers.\n",
    "* `instructions` defines specific instructions for the assistant, in this case asking it to address the user as a SISTCA student.\n",
    "* `event_handler` is the instance of the EventHandler class.\n",
    "\n",
    "The `with` block ensures that streaming continues until it is finished (stream.until_done())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With streaming\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "def streaming_run (thread_id, assistant_id, client):\n",
    "\n",
    "    # First, we create a EventHandler class to define\n",
    "    # how we want to handle the events in the response stream.\n",
    "    \n",
    "    class EventHandler(AssistantEventHandler):    \n",
    "        @override\n",
    "        def on_text_created(self, text) -> None:\n",
    "            print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "            \n",
    "        @override\n",
    "        def on_text_delta(self, delta, snapshot):\n",
    "            print(delta.value, end=\"\", flush=True)\n",
    "            \n",
    "        def on_tool_call_created(self, tool_call):\n",
    "            print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "        \n",
    "        def on_tool_call_delta(self, delta, snapshot):\n",
    "            if delta.type == 'code_interpreter':\n",
    "                if delta.code_interpreter.input:\n",
    "                    print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "\n",
    "    # Then, we use the `create_and_stream` SDK helper \n",
    "    # with the `EventHandler` class to create the Run \n",
    "    # and stream the response.\n",
    "    \n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id,\n",
    "        instructions=\"Please address the user as SISTCA student.\",\n",
    "        event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > SISTCA student, let's solve the equation `8x + 12 = 14` step by step.\n",
      "\n",
      "1. Subtract 12 from both sides:\n",
      "`8x = 14 - 12`\n",
      "`8x = 2`\n",
      "\n",
      "2. Divide by 8 to solve for x:\n",
      "`x = 2/8`\n",
      "`x = 1/4`\n",
      "\n",
      "Therefore, the solution to the equation `8x + 12 = 14` is `x = 1/4`.None\n"
     ]
    }
   ],
   "source": [
    "print(streaming_run(thread.id, assistant.id, client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.2 Function Calling\n",
    "\n",
    "Function Calling is one of Assistants API's tools, which in simple terms, invokes predefined functions in order to respond to a certain prompt. As you will verify in just a moment, in this tutorial we asked our weather assistant to give us the maximum and minimum temperatures and rain probability in a certain location.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using functions we allow the assistant to call methods the add extra functionality as it sees fit. \n",
    "\n",
    "For instance, in this case we want to make it so that when asking for weather data for a Portuguese city or region the data should come from [IPMA](https://www.ipma.pt/pt/index.html).\n",
    "\n",
    "By using [IPMA'S API](https://api.ipma.pt/) we can check the weather data for a specific location using it's ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the functions the assistant may call, alongside their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are a weather bot. Use the provided functions to answer questions.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather_data\",\n",
    "                \"description\": \"Get the current weather forecast for the specified location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City or region for which to get the weather forecast\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `get_location_id`, takes `location_name` as input and returns an id of the specific location. First of all, we create an `url`, which will be accessed in order to retrieve the actual data. To actually receive this data, we use a GET HTTP request, the `response` will then be converted to a json variable, in this case, `data`. As we now have access to all data retrieved from the GET request, now all we need is to just return the `location_id`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_location_id(location_name):\n",
    "    url = \"https://api.ipma.pt/open-data/distrits-islands.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for location in data[\"data\"]:\n",
    "                if location[\"local\"].lower() == location_name.lower():\n",
    "                    return str(location[\"globalIdLocal\"])\n",
    "            return None  # Location not found\n",
    "        else:\n",
    "            print(\"Failed to fetch data:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function, `get_weather_data()`, is very similar to `get_location_id()` as we simply use the returned variable, `location_id` to access the actual weather data of the required location, which it returns as `forecast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(location_id):\n",
    "    url = f\"https://api.ipma.pt/open-data/forecast/meteorology/cities/daily/{location_id}.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            forecast = data[\"data\"][0]\n",
    "            return forecast\n",
    "        else:\n",
    "            print(\"Failed to fetch data:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
