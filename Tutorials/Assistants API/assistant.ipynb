{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assistants API makes it possible to create AI assistants in applications. An Assistant has instructions and can use models, tools and files to answer the user's questions. The Assistants API currently supports three types of tools: Code Interpreter, File Search and Function Calling.\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.1 Assistant\n",
    "\n",
    "In this example, an Assistant is created who is a personal maths tutor.\n",
    "\n",
    "Initially, an Assistant is created, which represents an entity that can be configured to respond to a user's messages using various parameters such as a , *__name__*, *__instructions__* and *__model__*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from runners.standard_run import std_run\n",
    "from runners.streaming_run import streaming_run\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a Thread is created that represents a conversation between a user and one or more Assistants. A Thread can be created when a user (or their AI application) starts a conversation with their Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message is added to the Thread, the content of the messages that users or applications create is added as objects to the Message Thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `8x + 12 = 14`. Can you help me?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to create an execution, after all the user messages have been added to the Thread, you can execute the Thread with any assistant . Creating an Execution uses the model and tools associated with the assistant  to generate a response. These responses are added to the Thread as *__assistant__*  ard messages.\n",
    "\n",
    "The run can be with streaming or without streaming.\n",
    "\n",
    "\n",
    "In the run with streaming, the streaming_run function is defined, which is responsible for starting the streaming with the assistant. \n",
    "Within this is defined the 'EventHandler' class, which inherits from AssistantEventHandler. This class contains the methods for dealing with different events during streaming: \n",
    "\n",
    "* __'on_text_created':__ Called when a new text is created by the assistant. This prints 'assistant' to indicate that the assistant is responding.\n",
    "\n",
    "* __'on_text_delta':__ Called for each text update. This prints the part of the text that was generated (delta.value).\n",
    "\n",
    "* __'on_tool_call_created':__ Called when the wizard calls a tool. This prints the type of tool call (tool_call.type).\n",
    "\n",
    "* __'on_tool_call_delta':__  Called for tool call updates, specifically for the code_interpreter. This prints the code_interpreter input and any outputs, especially logs.\n",
    "\n",
    "\n",
    "Finally, the API client's create_and_stream method is used to start streaming, passing the EventHandler as the event handler.\n",
    "\n",
    "\n",
    "* __'thread_id'__ and __'assistant_id'__ are the required identifiers.\n",
    "* __'instructions'__ defines specific instructions for the assistant, in this case asking it to address the user as a SISTCA student.\n",
    "* __'event_handler'__ is the instance of the EventHandler class.\n",
    "\n",
    "The with block ensures that streaming continues until it is finished (stream.until_done())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With streaming\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "def streaming_run (thread_id, assistant_id, client):\n",
    "\n",
    "    # First, we create a EventHandler class to define\n",
    "    # how we want to handle the events in the response stream.\n",
    "    \n",
    "    class EventHandler(AssistantEventHandler):    \n",
    "        @override\n",
    "        def on_text_created(self, text) -> None:\n",
    "            print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "            \n",
    "        @override\n",
    "        def on_text_delta(self, delta, snapshot):\n",
    "            print(delta.value, end=\"\", flush=True)\n",
    "            \n",
    "        def on_tool_call_created(self, tool_call):\n",
    "            print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "        \n",
    "        def on_tool_call_delta(self, delta, snapshot):\n",
    "            if delta.type == 'code_interpreter':\n",
    "                if delta.code_interpreter.input:\n",
    "                    print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "\n",
    "    # Then, we use the `create_and_stream` SDK helper \n",
    "    # with the `EventHandler` class to create the Run \n",
    "    # and stream the response.\n",
    "    \n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id,\n",
    "        instructions=\"Please address the user as SISTCA student.\",\n",
    "        event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In run without streaming, you define the std_run function to start a new run in an existing thread, to wait for that run to complete and finally to list the messages in the thread and return the text of the first message if the run completes successfully, or return the run status otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without streaming\n",
    "\n",
    "def std_run (thread_id, assistant_id, client):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread_id,\n",
    "        assistant_id = assistant_id,\n",
    "        instructions = \"Please address the user as SISTCA student.\"\n",
    "    )   \n",
    "\n",
    "    if run.status == 'completed': \n",
    "\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id = thread_id\n",
    "        )\n",
    "\n",
    "        return(messages.data[0].content[0].text.value)        \n",
    "\n",
    "    else:\n",
    "        return(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the solution is printed with the desired run type:\n",
    "\n",
    "* __With streaming__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > Of course, SISTCA student! To solve the equation 8x + 12 = 14, we first need to isolate the variable x. Here's how you can do it:\n",
      "\n",
      "8x + 12 = 14\n",
      "\n",
      "First, subtract 12 from both sides:\n",
      "\n",
      "8x = 14 - 12\n",
      "8x = 2\n",
      "\n",
      "Now, divide both sides by 8 to solve for x:\n",
      "\n",
      "x = 2/8\n",
      "x = 1/4\n",
      "\n",
      "So, the solution to the equation 8x + 12 = 14 is x = 1/4. If you have any other questions or need further clarification, feel free to ask!None\n"
     ]
    }
   ],
   "source": [
    "print(streaming_run(thread.id, assistant.id, client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Without streaming__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, SISTCA student! I can help you solve the equation `8x + 12 = 14`.\n",
      "\n",
      "First, let's isolate the variable x by getting rid of the constant on the same side as x. We can do this by subtracting 12 from both sides of the equation:\n",
      "\n",
      "8x + 12 - 12 = 14 - 12\n",
      "8x = 2\n",
      "\n",
      "Next, divide by 8 on both sides to solve for x:\n",
      "\n",
      "8x/8 = 2/8\n",
      "x = 0.25\n",
      "\n",
      "So, the solution to the equation `8x + 12 = 14` is x = 0.25. If you have any more questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(std_run(thread.id, assistant.id, client))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
