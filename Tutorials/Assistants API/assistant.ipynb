{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assistants API makes it possible to create AI assistants in applications. An Assistant has instructions and can use models, tools and files to answer the user's questions. The Assistants API currently supports three types of tools: Code Interpreter, File Search and Function Calling.\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.1 Assistant\n",
    "\n",
    "In this example, an Assistant is created with the purpose of being a personal maths tutor.\n",
    "\n",
    "Initially, an Assistant is created, which represents an entity that can be configured to respond to a user's messages using various parameters such as a , *__name__*, *__instructions__* and *__model__*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from runners.standard_run import std_run\n",
    "from runners.streaming_run import streaming_run\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a Thread is created that represents a conversation between a user and one or more Assistants. A Thread can be created when a user (or their AI application) starts a conversation with their Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message is added to the Thread, the content of the messages that users or applications create is added as objects to the Message Thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `8x + 12 = 14`. Can you help me?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user has added all of the necessary information/messages to the thread you must run the thread to provide them with results.\n",
    "\n",
    "You can run a thread using its id as well as the assistant's id, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    #model = \"gpt-3.5-turbo\" # By default the model defined in the assistant will be used. You can, however, overwrite it here.\n",
    "    instructions = \"Please address the user as SISTCA student.\" # This parameter is optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During its lifecycle, a run can have multiple states:\n",
    "\n",
    "![image.png](https://cdn.openai.com/API/docs/images/diagram-run-statuses-v2.png)\n",
    "[https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps]\n",
    "\n",
    "Therefore, we must check for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queued\n"
     ]
    }
   ],
   "source": [
    "if run.status == 'completed': \n",
    "    messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "    )\n",
    "    print(messages)\n",
    "else:\n",
    "    print(run.status)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SISTCA student, let's solve the equation `8x + 12 = 14` step by step.\n",
      "\n",
      "1. Subtract 12 from both sides:\n",
      "`8x + 12 - 12 = 14 - 12`\n",
      "`8x = 2`\n",
      "\n",
      "2. Divide by 8 to solve for x:\n",
      "`8x / 8 = 2 / 8`\n",
      "`x = 1/4`\n",
      "\n",
      "Therefore, the solution to the equation `8x + 12 = 14` is `x = 1/4`.\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id = thread.id)\n",
    "print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to yield any results, an assistant's thread must be run. This is to say that whenever you're working with the assistant functionality you must have a runner function. \n",
    "\n",
    "Due to the fact that we will be exploring other types of assistants, and as a way of not repeating this code multiple times, we will export as the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_run (thread_id, assistant_id, client):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id = thread_id,\n",
    "        assistant_id = assistant_id,\n",
    "        instructions = \"Please address the user as SISTCA student.\"\n",
    "    )   \n",
    "\n",
    "    if run.status == 'completed': \n",
    "        messages = client.beta.threads.messages.list(thread_id = thread_id)\n",
    "        print(messages.data[0].content[0].text.value)\n",
    "    else:\n",
    "        print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have used ChatGPT before you may have noticed that the output is not returned all at once, like it is here, but rather gradually, in real time.\n",
    "\n",
    "We can achieve this same effect by using a different type of runner, the streaming run.\n",
    "\n",
    "\n",
    "In this run, the streaming_run function is defined, which is responsible for starting the streaming with the assistant. \n",
    "Within this, the 'EventHandler' class is defined, which is inherited from AssistantEventHandler. \n",
    "This class contains the methods responsible for dealing with the different events that occur during streaming: \n",
    "\n",
    "* `on_text_created`: Called when a new text is created by the assistant. This prints 'assistant' to indicate that the assistant is responding.\n",
    "\n",
    "* `on_text_delta`: Called for each text update. This prints the part of the text that was generated (delta.value).\n",
    "\n",
    "* `on_tool_call_created`: Called when the wizard calls a tool. This prints the type of tool call (tool_call.type).\n",
    "\n",
    "* `on_tool_call_delta`:  Called for tool call updates, specifically for the code_interpreter. This prints the code_interpreter input and any outputs, especially logs.\n",
    "\n",
    "\n",
    "Finally, the API client's create_and_stream method is used to start streaming, passing the EventHandler as the event handler.\n",
    "\n",
    "\n",
    "* __'thread_id`: and __'assistant_id'__ are the required identifiers.\n",
    "* __'instructions'__ defines specific instructions for the assistant, in this case asking it to address the user as a SISTCA student.\n",
    "* __'event_handler'__ is the instance of the EventHandler class.\n",
    "\n",
    "The `with` block ensures that streaming continues until it is finished (stream.until_done())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With streaming\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "\n",
    "def streaming_run (thread_id, assistant_id, client):\n",
    "\n",
    "    # First, we create a EventHandler class to define\n",
    "    # how we want to handle the events in the response stream.\n",
    "    \n",
    "    class EventHandler(AssistantEventHandler):    \n",
    "        @override\n",
    "        def on_text_created(self, text) -> None:\n",
    "            print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "            \n",
    "        @override\n",
    "        def on_text_delta(self, delta, snapshot):\n",
    "            print(delta.value, end=\"\", flush=True)\n",
    "            \n",
    "        def on_tool_call_created(self, tool_call):\n",
    "            print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "        \n",
    "        def on_tool_call_delta(self, delta, snapshot):\n",
    "            if delta.type == 'code_interpreter':\n",
    "                if delta.code_interpreter.input:\n",
    "                    print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "\n",
    "    # Then, we use the `create_and_stream` SDK helper \n",
    "    # with the `EventHandler` class to create the Run \n",
    "    # and stream the response.\n",
    "    \n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id,\n",
    "        instructions=\"Please address the user as SISTCA student.\",\n",
    "        event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > SISTCA student, let's solve the equation `8x + 12 = 14` step by step.\n",
      "\n",
      "1. Subtract 12 from both sides:\n",
      "`8x = 14 - 12`\n",
      "`8x = 2`\n",
      "\n",
      "2. Divide by 8 to solve for x:\n",
      "`x = 2/8`\n",
      "`x = 1/4`\n",
      "\n",
      "Therefore, the solution to the equation `8x + 12 = 14` is `x = 1/4`.None\n"
     ]
    }
   ],
   "source": [
    "print(streaming_run(thread.id, assistant.id, client))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
